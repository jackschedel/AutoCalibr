{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+mlBlGqxSalrtQClmjhaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackschedel/AutoCalibr/blob/main/AutoCalibr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset importing"
      ],
      "metadata": {
        "id": "gKVZUbqheVJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define imports and constants\n",
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# dataset:\n",
        "# fbx files (main dataset source)\n",
        "DATASET = '/content/drive/MyDrive/AutoCalibr/dataset/'\n",
        "\n",
        "# intermediate folders\n",
        "INTERMEDIATES = '/content/intermediates'\n",
        "CONVERTED_PLY = INTERMEDIATES + '/converted_ply/'\n",
        "NORMALIZED_PLY = INTERMEDIATES + '/normalized_ply/'\n",
        "\n",
        "# sometimes used for debug outputting into non-cluttered directory\n",
        "DIR = '/content/'\n",
        "\n",
        "# these will be automatically determined, do not change them (defined here to allow out-of-order exports)\n",
        "global_stretch_scale_x = 1\n",
        "global_stretch_scale_y = 1\n",
        "global_stretch_scale_z = 1\n",
        "\n",
        "!rm -r sample_data/ 2>/dev/null\n",
        "!mkdir {INTERMEDIATES} 2>/dev/null"
      ],
      "metadata": {
        "id": "SlrX3zBJ9YYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51d7732-ad7b-4938-95c5-0f3edcffcdef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert FBX to PLY {vertical-output: true}\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import bpy\n",
        "except ImportError:\n",
        "  !pip install bpy\n",
        "  print('-' * 50)\n",
        "  import bpy\n",
        "\n",
        "!rm -r {CONVERTED_PLY} 2>/dev/null\n",
        "!mkdir {CONVERTED_PLY}\n",
        "\n",
        "bpy.ops.wm.read_factory_settings()\n",
        "\n",
        "# needs to be rotated counterclockwise (left) 90 degrees - could be mostly automated (noted below)\n",
        "needs_extra_rotation = [\"Ace Of Spades\", \"Cantata-57\", \"Cloudstrike\", \"Dead Mans Tale\", \"Duality\", \"False Promises\", \"Fugue 55\", \"Hawkmoon\", \"Jack Queen King 3\", \"Mindbenders Ambition\", \"No Time To Explain\", \"Ruinous Effigy\", \"Seven Seraph Carbine\", \"Seventh Seraph CQC-12\", \"Seventh Seraph Officer Revolver\", \"Seventh Seraph SAW\", \"Seventh Seraph SI-2\", \"Seventh Seraph VY-7\", \"Trustee\", \"Witherhoard\"]\n",
        "\n",
        "fbx_dir = DATASET + 'fbx/'\n",
        "\n",
        "for f in os.listdir(fbx_dir):\n",
        "  if f.endswith('.fbx'):\n",
        "    # Isolate the name of the .fbx file (without extension)\n",
        "    name_no_ext = os.path.splitext(os.path.basename(f))[0]\n",
        "\n",
        "    print(f\"Object: {name_no_ext}\\n\")\n",
        "\n",
        "    # Delete all mesh objects to avoid exporting multiple models into the same file\n",
        "    bpy.ops.object.select_all(action='DESELECT')\n",
        "    bpy.ops.object.select_by_type(type='MESH')\n",
        "    bpy.ops.object.delete()\n",
        "\n",
        "    # Load in FBX file\n",
        "    bpy.ops.import_scene.fbx(filepath=os.path.join(fbx_dir, f))\n",
        "\n",
        "    # Select the object\n",
        "    obj_object = bpy.context.selected_objects[0]\n",
        "    bpy.context.view_layer.objects.active = obj_object\n",
        "\n",
        "    if name_no_ext in needs_extra_rotation:\n",
        "      forwards_dir = 'Z'\n",
        "      needs_extra_rotation.remove(name_no_ext)\n",
        "    else:\n",
        "      forwards_dir = '-X'\n",
        "\n",
        "    # Export object to PLY\n",
        "    bpy.ops.export_mesh.ply(filepath=os.path.join(CONVERTED_PLY, f.replace('.fbx', '.ply')), use_ascii=True, use_mesh_modifiers=True, use_normals=False, use_uv_coords=False, use_colors=False, axis_forward=forwards_dir, axis_up='Y')\n",
        "\n",
        "    print('-' * 50)\n",
        "\n",
        "# Ensure that any models that were supposed to receive extra rotation were hit\n",
        "if len(needs_extra_rotation) > 0:\n",
        "  print(f\"\\nThe following manually-specified models were not hit (check for typos): {needs_extra_rotation}\")"
      ],
      "metadata": {
        "id": "iG6Uc5bGslQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset processing"
      ],
      "metadata": {
        "id": "i79LmUiYMrJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define PlyObject Class\n",
        "import random\n",
        "import math\n",
        "from random import choice\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "def random_list_of_sum(length, total_sum):\n",
        "  adjusted_sum = total_sum - length\n",
        "  result = np.random.multinomial(adjusted_sum, np.ones(length)/length) + 1\n",
        "\n",
        "  return result.tolist()\n",
        "\n",
        "\n",
        "class Vertex:\n",
        "  def __init__(self, x, y, z):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.z = z\n",
        "\n",
        "\n",
        "  def distance(self, other_vertex):\n",
        "    diff_x = self.x - other_vertex.x\n",
        "    diff_y = self.y - other_vertex.y\n",
        "    diff_z = self.z - other_vertex.z\n",
        "\n",
        "    distance = math.sqrt(diff_x**2 + diff_y**2 + diff_z**2)\n",
        "    return distance\n",
        "\n",
        "\n",
        "  def to_list(self):\n",
        "    return [self.x, self.y, self.z]\n",
        "\n",
        "\n",
        "  def scale(self, scale_x, scale_y, scale_z):\n",
        "    self.x *= scale_x\n",
        "    self.y *= scale_y\n",
        "    self.z *= scale_z\n",
        "\n",
        "\n",
        "  def translate(self, offset_x, offset_y, offset_z):\n",
        "    self.x += offset_x\n",
        "    self.y += offset_y\n",
        "    self.z += offset_z\n",
        "\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash((self.x, self.y, self.z))\n",
        "\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    if isinstance(other, Vertex):\n",
        "      return self.x == other.x and self.y == other.y and self.z == other.z\n",
        "    return False\n",
        "\n",
        "\n",
        "class Face:\n",
        "  def __init__(self, vertices):\n",
        "    # vertices is a list of indexes (of the object's vertices list) of the connected vertices that form the face\n",
        "    self.vertices = vertices\n",
        "\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    for val1, val2 in zip(self.vertices, other.vertices):\n",
        "      if val1 < val2:\n",
        "        return True\n",
        "      elif val1 > val2:\n",
        "        return False\n",
        "    return len(self.vertices) < len(other.vertices)\n",
        "\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(tuple(self.vertices))\n",
        "\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    if isinstance(other, Face):\n",
        "      return (sorted(self.vertices) == sorted(other.vertices))\n",
        "    return False\n",
        "\n",
        "\n",
        "class PlyObject:\n",
        "  def __init__(self, name, vertices, faces):\n",
        "    self.name = name\n",
        "    self.vertices = vertices\n",
        "    self.faces = faces\n",
        "\n",
        "\n",
        "  def get_2d_vertex_list(self):\n",
        "    return np.array([[vertex.x, vertex.y, vertex.z] for vertex in self.vertices])\n",
        "\n",
        "\n",
        "  def get_2d_face_list(self):\n",
        "    return np.array([face.vertices for face in self.faces])\n",
        "\n",
        "\n",
        "  def get_2d_encoded_face_list(self):\n",
        "    # Create an empty 2D numpy array with the correct shape\n",
        "    encoded_faces = np.empty((len(self.faces) // 2, 3), dtype=np.int32)\n",
        "\n",
        "    # Process two faces at a time\n",
        "    for i in range(0, len(self.faces), 2):\n",
        "      face1 = self.faces[i]\n",
        "      face2 = self.faces[i + 1]\n",
        "\n",
        "      # Process each property (x, y, z)\n",
        "      for j in range(3):\n",
        "        # Cast to 16-bit, then encode together as a 32-bit value using alternating digits\n",
        "        value1_16 = np.int16(face1.vertices[j])\n",
        "        value2_16 = np.int16(face2.vertices[j])\n",
        "        encoded = np.bitwise_or.reduce(np.bitwise_or(np.left_shift(value1_16, np.arange(16)*2), np.left_shift(value2_16, np.arange(16)*2 + 1)))\n",
        "\n",
        "        # Store in the result array\n",
        "        encoded_faces[i // 2, j] = encoded\n",
        "\n",
        "    return encoded_faces\n",
        "\n",
        "\n",
        "  def get_2d_vertex_list(self):\n",
        "    vertex_list = []\n",
        "\n",
        "    for vertex in self.vertices:\n",
        "      vertex_list.append(vertex.to_list())\n",
        "\n",
        "    return vertex_list\n",
        "\n",
        "\n",
        "  def pad_with_random(self, max_vertices, max_faces):\n",
        "    self.subdivide_faces_as_padding(max_vertices, max_faces)\n",
        "\n",
        "    vertices_to_add = max_vertices - len(self.vertices)\n",
        "    faces_to_add = max_faces - len(self.faces)\n",
        "\n",
        "    if vertices_to_add > 0:\n",
        "      self.add_random_duplicate_vertices(vertices_to_add)\n",
        "    if faces_to_add > 0:\n",
        "      self.add_random_duplicate_faces(faces_to_add)\n",
        "\n",
        "    self.sort_tri_data()\n",
        "\n",
        "\n",
        "  def export_random_to_model(self, count, max_vertices, max_faces):\n",
        "    vertex_list_3d = np.zeros((count, max_vertices, 3))\n",
        "    face_list_3d_dual_encoded = np.zeros((count, (max_faces // 2), 3))\n",
        "\n",
        "    for i in range(count):\n",
        "      new_variation = copy.copy(self)\n",
        "\n",
        "      self.pad_with_random(max_vertices, max_faces)\n",
        "\n",
        "      vertex_list_3d[i] = new_variation.get_2d_vertex_list()\n",
        "      face_list_3d_dual_encoded[i] = new_variation.get_2d_encoded_face_list()\n",
        "\n",
        "    return { 'vertices': vertex_list_3d, 'faces': face_list_3d_dual_encoded }\n",
        "\n",
        "\n",
        "  def save_file(self, filename):\n",
        "    with open(filename, \"w\") as file:\n",
        "      file.write(\"ply\\n\")\n",
        "      file.write(\"format ascii 1.0\\n\")\n",
        "      file.write(\"comment Created by PlyObject class\\n\")\n",
        "      file.write(f\"element vertex {len(self.vertices)}\\n\")\n",
        "      file.write(\"property float x\\n\")\n",
        "      file.write(\"property float y\\n\")\n",
        "      file.write(\"property float z\\n\")\n",
        "      file.write(f\"element face {len(self.faces)}\\n\")\n",
        "      file.write(\"property list uchar uint vertex_indices\\n\")\n",
        "      file.write(\"end_header\\n\")\n",
        "\n",
        "      for vertex in self.vertices:\n",
        "        file.write(f\"{vertex.x} {vertex.y} {vertex.z}\\n\")\n",
        "\n",
        "      for face in self.faces:\n",
        "        formatted_vertices = ' '.join(str(v) for v in face.vertices)\n",
        "        file.write(f\"{len(face.vertices)} {formatted_vertices}\\n\")\n",
        "\n",
        "\n",
        "  def scale(self, scale_x, scale_y, scale_z):\n",
        "    for vertex in self.vertices:\n",
        "      vertex.scale(scale_x, scale_y, scale_z)\n",
        "\n",
        "\n",
        "  def translate(self, offset_x, offset_y, offset_z):\n",
        "    for vertex in self.vertices:\n",
        "      vertex.translate(offset_x, offset_y, offset_z)\n",
        "\n",
        "\n",
        "  def calculate_volume(self):\n",
        "    volume = 0\n",
        "    for face in self.faces:\n",
        "      v0 = self.vertices[face.vertices[0]]\n",
        "      v1 = self.vertices[face.vertices[1]]\n",
        "      v2 = self.vertices[face.vertices[2]]\n",
        "      volume += (-v0.x*v1.y*v2.z + v1.x*v0.y*v2.z + v0.x*v2.y*v1.z - v2.x*v0.y*v1.z + v2.x*v1.y*v0.z - v1.x*v2.y*v0.z)\n",
        "    return abs(volume) / 6.0\n",
        "\n",
        "\n",
        "  def remove_overlapping(self):\n",
        "    vert_dict = {}\n",
        "    convert_dict = {}\n",
        "\n",
        "    new_vertices = []\n",
        "    # iterate over existing vertices to identify and save unique ones\n",
        "    for idx, vertex in enumerate(self.vertices):\n",
        "      if vertex in vert_dict:\n",
        "        convert_dict[idx] = vert_dict[vertex]\n",
        "      else:\n",
        "        # assign a unique index to each vertex\n",
        "        vert_dict[vertex] = len(new_vertices)\n",
        "        convert_dict[idx] = len(new_vertices)\n",
        "        new_vertices.append(vertex)\n",
        "\n",
        "    # replace original vertices with new, duplicate-free list\n",
        "    self.vertices = new_vertices\n",
        "\n",
        "    # apply convert_dict to update face vertices to match new unique indexing\n",
        "    for face in self.faces:\n",
        "      face.vertices = [convert_dict[vertex] for vertex in face.vertices]\n",
        "\n",
        "    # convert faces to set to remove any potential duplicate faces\n",
        "    self.faces = set(self.faces)\n",
        "    self.faces = list(set(self.faces))\n",
        "\n",
        "\n",
        "  # not great for scale since re-sorting is needed, use a batch version after dataset processing\n",
        "  def add_random_duplicate_vertices(self, count_to_add):\n",
        "    # note: will need to re-sort tri data if already sorted\n",
        "    for _ in range(count_to_add):\n",
        "      to_duplicate = choice(self.vertices)\n",
        "      new_vertex = Vertex(to_duplicate.x, to_duplicate.y, to_duplicate.z)\n",
        "      self.vertices.append(new_vertex)\n",
        "\n",
        "\n",
        "  # not great for scale since re-sorting is needed, use a batch version after dataset processing\n",
        "  def add_random_duplicate_faces(self, count_to_add):\n",
        "    # note: will need to re-sort tri data if already sorted\n",
        "    for _ in range(count_to_add):\n",
        "      to_duplicate = choice(self.faces)\n",
        "      new_face = Face(to_duplicate.vertices)\n",
        "      self.faces.append(new_face)\n",
        "\n",
        "\n",
        "  def subdivide_face(self, face_index):\n",
        "    # Get the face to be subdivided\n",
        "    face = self.faces[face_index]\n",
        "\n",
        "    # Find vertices to split between and create new vertex in between\n",
        "    vertex_index_1, vertex_index_2 = face.vertices[0], face.vertices[1]\n",
        "    vertex_1, vertex_2 = self.vertices[vertex_index_1], self.vertices[vertex_index_2]\n",
        "\n",
        "    new_vertex = Vertex((vertex_1.x + vertex_2.x) / 2, (vertex_1.y + vertex_2.y) / 2, (vertex_1.z + vertex_2.z) / 2)\n",
        "\n",
        "    # Add new_vertex to the vertices list and store its index\n",
        "    self.vertices.append(new_vertex)\n",
        "    new_vertex_index = len(self.vertices) - 1\n",
        "\n",
        "    # Create two new faces with correct order to maintain outward normal\n",
        "    new_face_1 = Face([vertex_index_1, new_vertex_index, face.vertices[2]])\n",
        "    new_face_2 = Face([new_vertex_index, vertex_index_2, face.vertices[2]])\n",
        "\n",
        "    # Replace the old face with the new faces\n",
        "    self.faces[face_index] = new_face_1\n",
        "    self.faces.append(new_face_2)\n",
        "\n",
        "\n",
        "\n",
        "  def subdivide_faces_as_padding(self, max_vertices, max_faces):\n",
        "    # note: loses sorted status\n",
        "    while len(self.faces) < max_faces and len(self.vertices) < max_vertices:\n",
        "      face_index = random.randint(0, len(self.faces) - 1)\n",
        "      obj.subdivide_face(face_index)\n",
        "\n",
        "\n",
        "  def categorize_faces(self):\n",
        "    face_dict = {}\n",
        "\n",
        "    for face in obj.faces:\n",
        "      length = len(face.vertices)\n",
        "\n",
        "      if length in face_dict:\n",
        "        face_dict[length] = face_dict[length] + 1\n",
        "      else:\n",
        "        face_dict[length] = 1\n",
        "\n",
        "    return face_dict\n",
        "\n",
        "\n",
        "  def get_value_extrema(self):\n",
        "    min_val = float('inf')\n",
        "    max_val = float('-inf')\n",
        "\n",
        "    for v in self.vertices:\n",
        "      min_val = min(min_val, v.x, v.y, v.z)\n",
        "      max_val = max(max_val, v.x, v.y, v.z)\n",
        "\n",
        "    return {'min': min_val, 'max': max_val}\n",
        "\n",
        "\n",
        "  def get_max_values(self):\n",
        "    max_values = {'x': None, 'y': None, 'z': None}\n",
        "\n",
        "    for vertex in self.vertices:\n",
        "      if max_values['x'] is None or vertex.x > max_values['x']:\n",
        "        max_values['x'] = vertex.x\n",
        "      if max_values['y'] is None or vertex.y > max_values['y']:\n",
        "        max_values['y'] = vertex.y\n",
        "      if max_values['z'] is None or vertex.z > max_values['z']:\n",
        "        max_values['z'] = vertex.z\n",
        "\n",
        "    return max_values\n",
        "\n",
        "\n",
        "  def get_min_values(self):\n",
        "    min_values = {'x': None, 'y': None, 'z': None}\n",
        "\n",
        "    for vertex in self.vertices:\n",
        "      if min_values['x'] is None or vertex.x < min_values['x']:\n",
        "        min_values['x'] = vertex.x\n",
        "      if min_values['y'] is None or vertex.y < min_values['y']:\n",
        "        min_values['y'] = vertex.y\n",
        "      if min_values['z'] is None or vertex.z < min_values['z']:\n",
        "        min_values['z'] = vertex.z\n",
        "\n",
        "    return min_values\n",
        "\n",
        "\n",
        "  def center_object(self):\n",
        "    max_vals = self.get_max_values()\n",
        "    min_vals = self.get_min_values()\n",
        "\n",
        "    offset_vals = {'x': 0, 'y': 0, 'z': 0}\n",
        "\n",
        "    for dim in offset_vals:\n",
        "      center = (max_vals[dim] + min_vals[dim]) / 2\n",
        "      offset_vals[dim] = -center\n",
        "\n",
        "    self.translate(offset_vals['x'], offset_vals['y'], offset_vals['z'])\n",
        "\n",
        "\n",
        "  def normalize_scale(self):\n",
        "    x_coordinates = [vertex.x for vertex in self.vertices]\n",
        "    y_coordinates = [vertex.y for vertex in self.vertices]\n",
        "    z_coordinates = [vertex.z for vertex in self.vertices]\n",
        "\n",
        "    max_distance = max(x_coordinates + y_coordinates + z_coordinates)\n",
        "    min_distance = min(x_coordinates + y_coordinates + z_coordinates)\n",
        "    normalization_range = max_distance - min_distance\n",
        "\n",
        "    if normalization_range == 0:\n",
        "      raise ValueError(\"Normalization range cannot be zero\")\n",
        "\n",
        "    for vertex in self.vertices:\n",
        "      vertex.x = 2 * (vertex.x - min_distance) / normalization_range - 1\n",
        "      vertex.y = 2 * (vertex.y - min_distance) / normalization_range - 1\n",
        "      vertex.z = 2 * (vertex.z - min_distance) / normalization_range - 1\n",
        "\n",
        "\n",
        "  def squares_to_tris(self):\n",
        "    new_faces = []\n",
        "    for face in self.faces:\n",
        "      if len(face.vertices) == 4:\n",
        "        new_faces.append(Face([face.vertices[0], face.vertices[1], face.vertices[2]]))\n",
        "        new_faces.append(Face([face.vertices[0], face.vertices[2], face.vertices[3]]))\n",
        "      else:\n",
        "        new_faces.append(face)\n",
        "\n",
        "    self.faces = new_faces\n",
        "\n",
        "\n",
        "  def delete_plane(self):\n",
        "    # Note: only call if the object has a plane artifact!\n",
        "    max_x_index = max(range(len(self.vertices)), key = lambda index: self.vertices[index].x)\n",
        "    min_x_index = min(range(len(self.vertices)), key = lambda index: self.vertices[index].x)\n",
        "    max_z_index = max(range(len(self.vertices)), key = lambda index: self.vertices[index].z)\n",
        "    min_z_index = min(range(len(self.vertices)), key = lambda index: self.vertices[index].z)\n",
        "\n",
        "    indices_to_remove = set([max_x_index, min_x_index, max_z_index, min_z_index])\n",
        "\n",
        "    for f, face in enumerate(self.faces):\n",
        "      if set(face.vertices).intersection(indices_to_remove) == indices_to_remove:\n",
        "        del self.faces[f]\n",
        "        break\n",
        "    else:\n",
        "      raise ValueError(\"Face with all vertices not found\")\n",
        "\n",
        "    for index in sorted(indices_to_remove, reverse=True):\n",
        "      del self.vertices[index]\n",
        "\n",
        "    for face in self.faces:\n",
        "      face.vertices = [idx if idx not in indices_to_remove else -1 for idx in face.vertices]\n",
        "\n",
        "    self.remove_disconnected_vertices()\n",
        "\n",
        "\n",
        "  def remove_disconnected_vertices(self):\n",
        "    connected_vertices = set()\n",
        "    for face in self.faces:\n",
        "      connected_vertices |= set(face.vertices)\n",
        "\n",
        "    shift_indices = []\n",
        "    new_vertices = []\n",
        "    for idx, vertex in enumerate(self.vertices):\n",
        "      if idx in connected_vertices:\n",
        "        new_vertices.append(vertex)\n",
        "        shift_indices.append(len(new_vertices) - 1)\n",
        "      else:\n",
        "        shift_indices.append(None)\n",
        "\n",
        "    self.vertices = new_vertices\n",
        "\n",
        "    for face in self.faces:\n",
        "      face.vertices = [shift_indices[vertex] if shift_indices[vertex] is not None else None for vertex in face.vertices]\n",
        "      face.vertices = [vertex for vertex in face.vertices if vertex is not None]\n",
        "\n",
        "    self.faces = [face for face in self.faces if face.vertices]\n",
        "\n",
        "\n",
        "  def stretch_to_max(self):\n",
        "    # note: update scale globals before calling\n",
        "    self.scale(global_stretch_scale_x, global_stretch_scale_y, global_stretch_scale_z)\n",
        "\n",
        "\n",
        "  def revert_stretch(self):\n",
        "    self.scale(1/global_stretch_scale_x, 1/global_stretch_scale_y, 1/global_stretch_scale_z)\n",
        "\n",
        "\n",
        "  def dimension_range(self):\n",
        "    xmin = xmax = obj.vertices[0].x\n",
        "    ymin = ymax = obj.vertices[0].y\n",
        "    zmin = zmax = obj.vertices[0].z\n",
        "\n",
        "    for vertex in obj.vertices:\n",
        "      xmin = min(xmin, vertex.x)\n",
        "      xmax = max(xmax, vertex.x)\n",
        "      ymin = min(ymin, vertex.y)\n",
        "      ymax = max(ymax, vertex.y)\n",
        "      zmin = min(zmin, vertex.z)\n",
        "      zmax = max(zmax, vertex.z)\n",
        "\n",
        "    xrang = xmax - xmin\n",
        "    yrang = ymax - ymin\n",
        "    zrang = zmax - zmin\n",
        "\n",
        "    return {'x':xrang, 'y':yrang, 'z':zrang}\n",
        "\n",
        "\n",
        "  def sort_tri_data(self):\n",
        "    index_map = {}\n",
        "\n",
        "    # sort the vertices by x first, then y, then z\n",
        "    sorted_vertices = sorted(\n",
        "      enumerate(self.vertices),\n",
        "      key=lambda pair: (pair[1].x, pair[1].y, pair[1].z)\n",
        "    )\n",
        "    self.vertices = [pair[1] for pair in sorted_vertices]\n",
        "\n",
        "    # record the new indices of the vertices in the map\n",
        "    for i, pair in enumerate(sorted_vertices):\n",
        "      old_index, _ = pair\n",
        "      index_map[old_index] = i\n",
        "\n",
        "    # convert old face lists to new face lists using the index map\n",
        "    new_faces = []\n",
        "    for face in self.faces:\n",
        "      new_face_vertices = [index_map[i] for i in face.vertices]\n",
        "      # new_face_vertices.sort()\n",
        "      new_faces.append(Face(new_face_vertices))\n",
        "\n",
        "    # replace old face list with new face list, which we first sort\n",
        "    new_faces.sort()\n",
        "\n",
        "    self.faces = new_faces\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_file(cls, filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "      if next(f).strip() != \"ply\":\n",
        "        raise ValueError(\"The file being read is not a PLY file.\")\n",
        "\n",
        "      for _ in range(2):\n",
        "        next(f)\n",
        "\n",
        "      n_vertices = int(next(f).split()[-1])\n",
        "\n",
        "      for _ in range(3):\n",
        "        next(f)\n",
        "\n",
        "      n_faces = int(next(f).split()[-1])\n",
        "\n",
        "      for _ in range(2):\n",
        "        next(f)\n",
        "\n",
        "      vertices = []\n",
        "      for _ in range(n_vertices):\n",
        "        x, y, z = map(float, next(f).split())\n",
        "        vertices.append(Vertex(x, y, z))\n",
        "\n",
        "      faces = []\n",
        "      for _ in range(n_faces):\n",
        "        face_vertices = list(map(int, next(f).split()[1:]))\n",
        "        faces.append(Face(face_vertices))\n",
        "\n",
        "    name = os.path.splitext(os.path.basename(filepath))[0]\n",
        "\n",
        "    return cls(name, vertices, faces)"
      ],
      "metadata": {
        "id": "xwXw3NRMICxb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Process PLY files as PlyObject {vertical-output: true}\n",
        "\n",
        "import os\n",
        "\n",
        "# Toggle between using full dataset and just preconverted subset\n",
        "use_fbx_dataset = False\n",
        "use_ply_dataset = True\n",
        "# if both are false, it will use a single test file from the ply dataset\n",
        "\n",
        "ply_objs = []\n",
        "\n",
        "if use_fbx_dataset:\n",
        "  for f in os.listdir(CONVERTED_PLY):\n",
        "    if f.endswith('.ply'):\n",
        "      start_time = time.time()\n",
        "      obj = PlyObject.from_file(filepath=os.path.join(CONVERTED_PLY, f))\n",
        "      ply_objs.append(obj)\n",
        "      elapsed_time = time.time() - start_time\n",
        "\n",
        "      print(f\"Object: {obj.name}\\n\")\n",
        "      print(f'Processed auto-converted PLY into PlyObject (took {elapsed_time:.2f} s)')\n",
        "      print(f'\\nVertice count: {len(obj.vertices)}')\n",
        "      print(f'Face count: {len(obj.faces)}')\n",
        "      print('-' * 50)\n",
        "\n",
        "\n",
        "# Import pre-converted .ply files\n",
        "if use_ply_dataset:\n",
        "  for f in os.listdir(DATASET+'ply/'):\n",
        "    if f.endswith('.ply'):\n",
        "      start_time = time.time()\n",
        "      obj = PlyObject.from_file(filepath=os.path.join(DATASET+'ply/', f))\n",
        "      ply_objs.append(obj)\n",
        "      elapsed_time = time.time() - start_time\n",
        "\n",
        "      print(f\"Object: {obj.name}\\n\")\n",
        "      print(f'Processed pre-converted PLY into PlyObject (took {elapsed_time:.2f} s)')\n",
        "      print(f'\\nVertice count: {len(obj.vertices)}')\n",
        "      print(f'Face count: {len(obj.faces)}')\n",
        "      print('-' * 50)\n",
        "\n",
        "if not (use_ply_dataset or use_fbx_dataset):\n",
        "  f = DATASET+'ply/'+'cube.ply'\n",
        "  if not os.path.isfile(f):\n",
        "    raise Exception(f'{f} not found!')\n",
        "\n",
        "  start_time = time.time()\n",
        "  obj = PlyObject.from_file(filepath=os.path.join(DATASET+'ply/', f))\n",
        "  ply_objs.append(obj)\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  print(f\"Object: {obj.name}\\n\")\n",
        "  print(f'Processed pre-converted PLY into PlyObject (took {elapsed_time:.2f} s)')\n",
        "  print(f'\\nVertice count: {len(obj.vertices)}')\n",
        "  print(f'Face count: {len(obj.faces)}')\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "ArkHJMUYLgjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39fd189b-31b8-4611-afed-36f841b4417e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.26 s)\n",
            "\n",
            "Vertice count: 24644\n",
            "Face count: 29692\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.06 s)\n",
            "\n",
            "Vertice count: 12219\n",
            "Face count: 24308\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.59 s)\n",
            "\n",
            "Vertice count: 86406\n",
            "Face count: 28802\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.07 s)\n",
            "\n",
            "Vertice count: 13902\n",
            "Face count: 24149\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.04 s)\n",
            "\n",
            "Vertice count: 13044\n",
            "Face count: 14070\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Remove plane artifacts (dataset specific cleaning) {vertical-output: true}\n",
        "\n",
        "# has a giant rectangular plane originally used as a background, will need to be filtered out\n",
        "has_plane_artifact = ['Abbadon', 'Blind Perdition', 'Ex Machina', 'Komodo-4FR', 'Nova Mortis', 'Trespasser', 'Vestian Dynasty', 'Vouchsafe', 'Hereafter']\n",
        "\n",
        "# Remove plane artifact from manually specified objects\n",
        "\n",
        "# Could also iterate over all objects and use exceptions,\n",
        "# but would need to give delete_plane function stricter pre-deletion checking\n",
        "for obj in ply_objs:\n",
        "  if obj.name in has_plane_artifact:\n",
        "    has_plane_artifact.remove(obj.name)\n",
        "\n",
        "    obj_range = obj.dimension_range()\n",
        "\n",
        "    print(f\"Object: {obj.name}\")\n",
        "    print(\"\\nDimension Range:\")\n",
        "    print('X:', obj_range['x'])\n",
        "    print('Y:', obj_range['y'])\n",
        "    print('Z:', obj_range['z'])\n",
        "\n",
        "    try:\n",
        "      start_time = time.time()\n",
        "\n",
        "      obj.delete_plane()\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "\n",
        "      obj_range = obj.dimension_range()\n",
        "\n",
        "      print(f\"\\nPlane detected and deleted (took {elapsed_time:.2f} s)\")\n",
        "      print(f\"\\nDimension Range (plane deleted in {elapsed_time:.2f} s):\")\n",
        "      print('X:', obj_range['x'])\n",
        "      print('Y:', obj_range['y'])\n",
        "      print('Z:', obj_range['z'])\n",
        "    except ValueError:\n",
        "      print(f\"\\nNo plane found!\")\n",
        "\n",
        "    print('-' * 50)\n",
        "\n",
        "if len(has_plane_artifact) > 0:\n",
        "  print(f\"\\nThe following manually-specified models were not hit (check for typos): {has_plane_artifact}\")"
      ],
      "metadata": {
        "id": "0_KXsecmwm8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74371895-713e-435d-a092-4008aa02264b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Blind Perdition\n",
            "\n",
            "Dimension Range:\n",
            "X: 1.831408\n",
            "Y: 0.47424900000000003\n",
            "Z: 3.43575\n",
            "\n",
            "Plane detected and deleted (took 0.06 s)\n",
            "\n",
            "Dimension Range (plane deleted in 0.06 s):\n",
            "X: 0.10221\n",
            "Y: 0.46234\n",
            "Z: 1.319475\n",
            "--------------------------------------------------\n",
            "\n",
            "The following manually-specified models were not hit (check for typos): ['Abbadon', 'Ex Machina', 'Komodo-4FR', 'Nova Mortis', 'Trespasser', 'Vestian Dynasty', 'Vouchsafe', 'Hereafter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Categorize face data to check for any bad n-gons\n",
        "\n",
        "face_lengths = {}\n",
        "\n",
        "for obj in ply_objs:\n",
        "  temp_face_lengths = obj.categorize_faces()\n",
        "\n",
        "  for length, count in temp_face_lengths.items():\n",
        "    if length in face_lengths:\n",
        "      face_lengths[length] += count\n",
        "    else:\n",
        "      face_lengths[length] = count\n",
        "\n",
        "for length, count in face_lengths.items():\n",
        "  print(f'Faces with {length} vertices: {count} instances')\n",
        "\n",
        "for length, count in face_lengths.items():\n",
        "  if length < 3 or length > 4:\n",
        "    raise Exception(f'\\nFace of unsupported size {length}!')"
      ],
      "metadata": {
        "id": "k-6q7ECD6Laa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3402c1ec-aea0-4df0-fde8-ad2ed1aff845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faces with 3 vertices: 119728 instances\n",
            "Faces with 4 vertices: 1292 instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert any objects with square faces to tris {vertical-output: true}\n",
        "\n",
        "# this could probably done using bpy before exporting as a PLY but this allows this to be done for any PlyObject\n",
        "\n",
        "face_lengths = {}\n",
        "square_obj_count = 0\n",
        "for obj in ply_objs:\n",
        "\n",
        "  face_data = obj.categorize_faces()\n",
        "\n",
        "  if 4 in face_data:\n",
        "    square_obj_count = square_obj_count + 1\n",
        "\n",
        "    print(f\"Object: {obj.name}\\n\")\n",
        "    if 3 in face_data:\n",
        "      print(f\"Tris: {face_data[3]}\")\n",
        "    else:\n",
        "      print(f\"Squares: 0\")\n",
        "    print(f\"Squares: {face_data[4]}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    obj.squares_to_tris()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f'\\nConverted square faces to tris (took {elapsed_time:.2f} s)\\n')\n",
        "    face_data = obj.categorize_faces()\n",
        "    print(f\"Tris: {face_data[3]}\")\n",
        "    if 4 in face_data:\n",
        "      print(f\"Squares: {face_data[4]}\")\n",
        "    else:\n",
        "      print(f\"Squares: 0\")\n",
        "\n",
        "    print('-' * 50)\n",
        "\n",
        "if square_obj_count == 0:\n",
        "  print(f\"No objects containing squares found. All objects contain only tris.\")"
      ],
      "metadata": {
        "id": "Px_DDxfNe-gk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a5226e-60b0-4913-9131-c44748eb5714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Prometheus Lens\n",
            "\n",
            "Tris: 22857\n",
            "Squares: 1292\n",
            "\n",
            "Converted square faces to tris (took 0.01 s)\n",
            "\n",
            "Tris: 25441\n",
            "Squares: 0\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Merge overlapping/duplicate vertices and faces {vertical-output: true}\n",
        "\n",
        "for obj in ply_objs:\n",
        "  initial_vertex_count = len(obj.vertices)\n",
        "  initial_face_count = len(obj.faces)\n",
        "\n",
        "  print(f\"Object: {obj.name}\\n\")\n",
        "  print(f'Vertex count: {initial_vertex_count}')\n",
        "  print(f'Face count: {initial_face_count}')\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  obj.remove_overlapping()\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  print(f'\\nDuplicate vertices: {initial_vertex_count - len(obj.vertices)}')\n",
        "  print(f'Duplicate faces: {initial_face_count - len(obj.faces)}')\n",
        "  print(f'\\nMerged any overlaping vertices and faces (took {elapsed_time:.2f} s)')\n",
        "  print(f'\\nVertex count: {len(obj.vertices)}')\n",
        "  print(f'Face count: {len(obj.faces)}')\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "xi9CjvimyRRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e03eb1-4ba9-479f-c206-1608464c18ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Vertex count: 24644\n",
            "Face count: 29692\n",
            "\n",
            "Duplicate vertices: 8148\n",
            "Duplicate faces: 3\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.09 s)\n",
            "\n",
            "Vertex count: 16496\n",
            "Face count: 29689\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Vertex count: 12219\n",
            "Face count: 24308\n",
            "\n",
            "Duplicate vertices: 0\n",
            "Duplicate faces: 0\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.05 s)\n",
            "\n",
            "Vertex count: 12219\n",
            "Face count: 24308\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Vertex count: 86406\n",
            "Face count: 28802\n",
            "\n",
            "Duplicate vertices: 78934\n",
            "Duplicate faces: 609\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.18 s)\n",
            "\n",
            "Vertex count: 7472\n",
            "Face count: 28193\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Vertex count: 13902\n",
            "Face count: 25441\n",
            "\n",
            "Duplicate vertices: 1312\n",
            "Duplicate faces: 44\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.05 s)\n",
            "\n",
            "Vertex count: 12590\n",
            "Face count: 25397\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Vertex count: 13040\n",
            "Face count: 14069\n",
            "\n",
            "Duplicate vertices: 5233\n",
            "Duplicate faces: 0\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.03 s)\n",
            "\n",
            "Vertex count: 7807\n",
            "Face count: 14069\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Center the objects to the origin {vertical-output: true}\n",
        "\n",
        "for obj in ply_objs:\n",
        "    start_time = time.time()\n",
        "\n",
        "    obj.center_object()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Object: {obj.name}\\n\")\n",
        "\n",
        "    print(f'Centered to origin (took {elapsed_time:.2f} s)')\n",
        "\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "5kHJGPW1a3ks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9727642-c7ee-4fb2-e694-f7461e57d839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Centered to origin (took 0.03 s)\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Centered to origin (took 0.01 s)\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Centered to origin (took 0.02 s)\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Centered to origin (took 0.02 s)\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Centered to origin (took 0.01 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Normalize ondividual object scale to perfectly fit boundaries {vertical-output: true}\n",
        "\n",
        "for obj in ply_objs:\n",
        "\n",
        "  extrema = obj.get_value_extrema()\n",
        "\n",
        "  print(f\"Object: {obj.name}\")\n",
        "  print(\"\\nExtrema in any Dimension:\")\n",
        "  print(f\"Minimum: {extrema['min']}\")\n",
        "  print(f\"Maximum: {extrema['max']}\")\n",
        "\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  obj.normalize_scale()\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  print(f'\\nNormalised object scale to boundaries (took {elapsed_time:.2f} s)')\n",
        "\n",
        "  extrema = obj.get_value_extrema()\n",
        "\n",
        "  print(\"\\nExtrema in any Dimension:\")\n",
        "  print(f\"Minimum: {extrema['min']}\")\n",
        "  print(f\"Maximum: {extrema['max']}\")\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "kdzIJHgumSYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18008890-e407-49ee-bf94-9bad346b5292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "\n",
            "Normalised object scale to boundaries (took 0.02 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -0.1960095\n",
            "Maximum: 0.1960095\n",
            "\n",
            "Normalised object scale to boundaries (took 0.02 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -0.4440425\n",
            "Maximum: 0.4440425\n",
            "\n",
            "Normalised object scale to boundaries (took 0.01 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "\n",
            "Normalised object scale to boundaries (took 0.02 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -0.6597375\n",
            "Maximum: 0.6597375\n",
            "\n",
            "Normalised object scale to boundaries (took 0.01 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate minimum volume of any object\n",
        "\n",
        "volumes = {}\n",
        "min_volume = float('inf')\n",
        "volume_calc_times = {}\n",
        "\n",
        "for obj in ply_objs:\n",
        "    start_time = time.time()\n",
        "\n",
        "    volumes[obj.name] = obj.calculate_volume()\n",
        "\n",
        "    volume_calc_times[obj.name] = time.time() - start_time\n",
        "\n",
        "    min_volume = min(min_volume, volumes[obj.name])\n",
        "\n",
        "print(f'Global minimum volume: {min_volume:.6f}')"
      ],
      "metadata": {
        "id": "jnfWd69JS7rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50089940-4761-40c5-cbdc-06225391acf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global minimum volume: 0.028479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Scale each object to match the minimum global volume {vertical-output: true}\n",
        "\n",
        "# just scaled to boundaries so the objects must be scaled down, not up\n",
        "\n",
        "import math\n",
        "\n",
        "for obj in ply_objs:\n",
        "    print(f\"Object: {obj.name}\\n\")\n",
        "    print(f'Volume calculated as: {volumes[obj.name]:.6f} (took {volume_calc_times[obj.name]:.2f} s)')\n",
        "    start_time = time.time()\n",
        "\n",
        "    to_scale = math.pow(min_volume / volumes[obj.name], 1/3)\n",
        "    obj.scale(to_scale, to_scale, to_scale)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'\\nScaled by {to_scale} (took {elapsed_time:.2f} s)')\n",
        "    start_time = time.time()\n",
        "\n",
        "    volume = obj.calculate_volume()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'\\nVolume calculated as: {volume:.6f} (took {elapsed_time:.2f} s)')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "tvQGDyWRMYBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2da568-ec5a-470c-fbc1-1d468e73385f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.08 s)\n",
            "\n",
            "Scaled by 1.0 (took 0.01 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.07 s)\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Volume calculated as: 0.272199 (took 0.06 s)\n",
            "\n",
            "Scaled by 0.47121092908297674 (took 0.01 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.05 s)\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Volume calculated as: 0.065950 (took 0.07 s)\n",
            "\n",
            "Scaled by 0.7558567953805982 (took 0.01 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.07 s)\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Volume calculated as: 0.050188 (took 0.05 s)\n",
            "\n",
            "Scaled by 0.8278999959990196 (took 0.01 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.06 s)\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Volume calculated as: 0.044829 (took 0.03 s)\n",
            "\n",
            "Scaled by 0.8596580597884671 (took 0.00 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.03 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Analyze dimension ranges to pseudo-verify object orientation\n",
        "\n",
        "# could do anywhere before doing global stretch\n",
        "# choosing to do it after all other processing\n",
        "\n",
        "# used for checking if I missed any rotation overrides\n",
        "count_smallest = {'x': 0, 'y': 0, 'z': 0}\n",
        "count_middle = {'x': 0, 'y': 0, 'z': 0}\n",
        "count_largest = {'x': 0, 'y': 0, 'z': 0}\n",
        "\n",
        "for obj in ply_objs:\n",
        "\n",
        "  obj_range = obj.dimension_range()\n",
        "\n",
        "  # count the podium placings of ranges for every dimension\n",
        "  sorted_keys = sorted(obj_range, key=obj_range.get)\n",
        "  smallest_key = sorted_keys[0]\n",
        "  middle_key = sorted_keys[1]\n",
        "  largest_key = sorted_keys[2]\n",
        "\n",
        "  count_smallest[smallest_key] += 1\n",
        "  count_middle[middle_key] += 1\n",
        "  count_largest[largest_key] += 1\n",
        "\n",
        "print(\"Times with largest dimensional range:\")\n",
        "print(f\"X: {count_largest['x']}\")\n",
        "print(f\"Y: {count_largest['y']}\")\n",
        "print(f\"Z: {count_largest['z']}\")\n",
        "\n",
        "print(\"\\nTimes with middle dimensional range:\")\n",
        "print(f\"X: {count_middle['x']}\")\n",
        "print(f\"Y: {count_middle['y']}\")\n",
        "print(f\"Z: {count_middle['z']}\")\n",
        "\n",
        "print(\"\\nTimes with smallest dimensional range:\")\n",
        "print(f\"X: {count_smallest['x']}\")\n",
        "print(f\"Y: {count_smallest['y']}\")\n",
        "print(f\"Z: {count_smallest['z']}\")"
      ],
      "metadata": {
        "id": "G_IkGyZ3Og6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e5af7f-7b2e-4deb-bf38-95c77188f5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Times with largest dimensional range:\n",
            "X: 0\n",
            "Y: 0\n",
            "Z: 5\n",
            "\n",
            "Times with middle dimensional range:\n",
            "X: 0\n",
            "Y: 5\n",
            "Z: 0\n",
            "\n",
            "Times with smallest dimensional range:\n",
            "X: 5\n",
            "Y: 0\n",
            "Z: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate global stretch values given global dimensional extrema\n",
        "\n",
        "min_x = min_y = min_z = float('inf')\n",
        "max_x = max_y = max_z = float('-inf')\n",
        "\n",
        "for obj in ply_objs:\n",
        "  for vertex in obj.vertices:\n",
        "    min_x = min(min_x, vertex.x)\n",
        "    min_y = min(min_y, vertex.y)\n",
        "    min_z = min(min_z, vertex.z)\n",
        "    max_x = max(max_x, vertex.x)\n",
        "    max_y = max(max_y, vertex.y)\n",
        "    max_z = max(max_z, vertex.z)\n",
        "\n",
        "scale_x, scale_y, scale_z = 2/(max_x - min_x), 2/(max_y - min_y), 2/(max_z - min_z)\n",
        "\n",
        "print(f\"Dimensional minima:\")\n",
        "print(f\"X: {min_x}\")\n",
        "print(f\"Y: {min_y}\")\n",
        "print(f\"Z: {min_z}\")\n",
        "\n",
        "print(f\"\\nDimensional maxima:\")\n",
        "print(f\"X: {max_x}\")\n",
        "print(f\"Y: {max_y}\")\n",
        "print(f\"Z: {max_z}\\n\")\n",
        "\n",
        "print('-' * 50)\n",
        "\n",
        "print(f\"\\nDerived Stretch Value:\")\n",
        "print(f\"X: {scale_x}\")\n",
        "print(f\"Y: {scale_y}\")\n",
        "print(f\"Z: {scale_z}\")"
      ],
      "metadata": {
        "id": "zSFUwzZuOFvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38607d30-144f-4577-9a23-3251bda117b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensional minima:\n",
            "X: -0.12194939584895402\n",
            "Y: -0.33554149999999994\n",
            "Z: -1.0\n",
            "\n",
            "Dimensional maxima:\n",
            "X: 0.12194939584895394\n",
            "Y: 0.33554149999999994\n",
            "Z: 1.0\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Derived Stretch Value:\n",
            "X: 8.200122624950072\n",
            "Y: 2.9802572856114673\n",
            "Z: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Stretch non-bounded dimensions using constant values to tighten scope {vertical-output: true}\n",
        "\n",
        "global_stretch_scale_x = scale_x\n",
        "global_stretch_scale_y = scale_y\n",
        "global_stretch_scale_z = scale_z\n",
        "\n",
        "for obj in ply_objs:\n",
        "  obj_range = obj.dimension_range()\n",
        "\n",
        "  print(f\"Object: {obj.name}\")\n",
        "  print(\"\\nDimension Range:\")\n",
        "  print('X:', obj_range['x'])\n",
        "  print('Y:', obj_range['y'])\n",
        "  print('Z:', obj_range['z'])\n",
        "\n",
        "  start_time = time.time()\n",
        "  obj.stretch_to_max()\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  obj_range = obj.dimension_range()\n",
        "\n",
        "  print(f\"\\nStretched non-bounded dimensions using global constants (took {elapsed_time:.2f} s):\")\n",
        "  print(f\"\\nDimension Range (stretched in {elapsed_time:.2f} s):\")\n",
        "  print('X:', obj_range['x'])\n",
        "  print('Y:', obj_range['y'])\n",
        "  print('Z:', obj_range['z'])\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "YBSTpRUREVt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c6f6a3a-a0eb-46da-da44-96bc42829b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.14593999999999996\n",
            "Y: 0.6710829999999999\n",
            "Z: 2.0\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.01 s):\n",
            "\n",
            "Dimension Range (stretched in 0.01 s):\n",
            "X: 1.1967258958852132\n",
            "Y: 2.0\n",
            "Z: 2.0\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.14158240522736318\n",
            "Y: 0.5979953838336013\n",
            "Z: 0.9424218581659535\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.01 s):\n",
            "\n",
            "Dimension Range (stretched in 0.01 s):\n",
            "X: 1.1609930843997502\n",
            "Y: 1.7821800994321162\n",
            "Z: 0.9424218581659535\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.24389879169790796\n",
            "Y: 0.5689507899464725\n",
            "Z: 1.5117135907611965\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.01 s):\n",
            "\n",
            "Dimension Range (stretched in 0.01 s):\n",
            "X: 2.0\n",
            "Y: 1.6956197368923742\n",
            "Z: 1.5117135907611965\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.12211874839298595\n",
            "Y: 0.5537732888229542\n",
            "Z: 1.6557999919980393\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.01 s):\n",
            "\n",
            "Dimension Range (stretched in 0.01 s):\n",
            "X: 1.0013887116279094\n",
            "Y: 1.6503868785916325\n",
            "Z: 1.6557999919980393\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.13318274357752768\n",
            "Y: 0.602443104056689\n",
            "Z: 1.7193161195769342\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.00 s):\n",
            "\n",
            "Dimension Range (stretched in 0.00 s):\n",
            "X: 1.0921148288630087\n",
            "Y: 1.7954354500313348\n",
            "Z: 1.7193161195769342\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Offset objects to pin Z and Y object maximums to upper boundaries to localize similar features (dataset specific scope reduction) {vertical-output: true}\n",
        "\n",
        "# pinning all objects against back wall\n",
        "\n",
        "for obj in ply_objs:\n",
        "  start_time = time.time()\n",
        "\n",
        "  max_values = obj.get_max_values()\n",
        "\n",
        "  max_z = max_values['z']\n",
        "  z_offset = 1 - max_z\n",
        "\n",
        "  max_y = max_values['y']\n",
        "  y_offset = 1 - max_y\n",
        "\n",
        "  obj.translate(0, y_offset, z_offset)\n",
        "\n",
        "  max_values = obj.get_max_values()\n",
        "  new_max_z = max_values['z']\n",
        "  new_max_y = max_values['y']\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  print(f\"Object: {obj.name}\")\n",
        "  print(f\"\\nMaximum Z: {max_z}\")\n",
        "  print(f\"\\nMaximum Y: {max_y}\")\n",
        "  print(f'\\nOffset object to boundaries (took {elapsed_time:.2f} s)')\n",
        "  print(f\"\\nMaximum Z: {new_max_z}\")\n",
        "  print(f\"\\nMaximum Y: {new_max_y}\")\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "-ZOVWrpTsFpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78fe6415-7dcd-403f-91ac-ad92ac3e6c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "\n",
            "Offset object to boundaries (took 0.03 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Maximum Z: 0.47121092908297674\n",
            "\n",
            "Maximum Y: 0.8910900497160581\n",
            "\n",
            "Offset object to boundaries (took 0.02 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Maximum Z: 0.7558567953805982\n",
            "\n",
            "Maximum Y: 0.8478098684461871\n",
            "\n",
            "Offset object to boundaries (took 0.02 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Maximum Z: 0.8278999959990196\n",
            "\n",
            "Maximum Y: 0.8251934392958161\n",
            "\n",
            "Offset object to boundaries (took 0.02 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Maximum Z: 0.8596580597884671\n",
            "\n",
            "Maximum Y: 0.8977177250156672\n",
            "\n",
            "Offset object to boundaries (took 0.02 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Verify that dataset dimensional extrema are properly constrained\n",
        "\n",
        "track_min = {'x': float('inf'), 'y': float('inf'), 'z': float('inf')}\n",
        "track_max = {'x': float('-inf'), 'y': float('-inf'), 'z': float('-inf')}\n",
        "\n",
        "for obj in ply_objs:\n",
        "  max_values = obj.get_max_values()\n",
        "  min_values = obj.get_min_values()\n",
        "\n",
        "  for dimension in ['x', 'y', 'z']:\n",
        "    track_min[dimension] = min(track_min[dimension], min_values[dimension])\n",
        "    track_max[dimension] = max(track_max[dimension], max_values[dimension])\n",
        "\n",
        "print(\"Dimensional Minima:\")\n",
        "print('X:', track_min['x'])\n",
        "print('Y:', track_min['y'])\n",
        "print('Z:', track_min['z'])\n",
        "\n",
        "print(\"\\nDimensional Maxima:\")\n",
        "print('X:', track_max['x'])\n",
        "print('Y:', track_max['y'])\n",
        "print('Z:', track_max['z'])"
      ],
      "metadata": {
        "id": "mZkkQKFtbXBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8984f04e-81e7-49d2-da84-41e0ff159088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensional Minima:\n",
            "X: -1.0000000000000002\n",
            "Y: -1.0\n",
            "Z: -1.0\n",
            "\n",
            "Dimensional Maxima:\n",
            "X: 0.9999999999999997\n",
            "Y: 1.0\n",
            "Z: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sort order of vertices and faces numerically to remove arbitrary sample noise {vertical-output: true}\n",
        "\n",
        "for obj in ply_objs:\n",
        "    start_time = time.time()\n",
        "\n",
        "    obj.sort_tri_data()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Object: {obj.name}\\n\")\n",
        "\n",
        "    print(f'Sorted object vertices and faces numerically (took {elapsed_time:.2f} s)')\n",
        "\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "cLgEiIWuPW1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10d1c1d-9c60-4f42-8bc0-4b46b31501b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Sorted object vertices and faces numerically (took 0.89 s)\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Sorted object vertices and faces numerically (took 0.37 s)\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Sorted object vertices and faces numerically (took 1.00 s)\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Sorted object vertices and faces numerically (took 0.39 s)\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Sorted object vertices and faces numerically (took 0.54 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export normalized PLY files {vertical-output: true}\n",
        "\n",
        "!rm -r {NORMALIZED_PLY} 2>/dev/null\n",
        "!mkdir {NORMALIZED_PLY}\n",
        "\n",
        "for obj in ply_objs:\n",
        "  start_time = time.time()\n",
        "\n",
        "  # todo: remove before final model use\n",
        "  obj_copy = copy.copy(obj)\n",
        "  obj_copy.revert_stretch()\n",
        "\n",
        "  obj_copy.save_file(NORMALIZED_PLY + obj.name + '.ply')\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "\n",
        "  print(f\"Object: {obj.name}\\n\")\n",
        "  print(f'Exported normalized PLY file (took {elapsed_time:.2f} s)')\n",
        "  print('-' * 50)\n",
        "\n",
        "# note - resets normalized_ply directory in drive if set to True\n",
        "send_to_google_drive = False\n",
        "if send_to_google_drive:\n",
        "  !rm -f {DATASET + 'normalized_ply/*'} 2>/dev/null\n",
        "  !cp -r {NORMALIZED_PLY} {DATASET}"
      ],
      "metadata": {
        "id": "0RtTCSIT5OmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76243521-bc12-4b0d-8d44-7e9c88060c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Exported normalized PLY file (took 0.24 s)\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Exported normalized PLY file (took 0.15 s)\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Exported normalized PLY file (took 0.14 s)\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Exported normalized PLY file (took 0.16 s)\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Exported normalized PLY file (took 0.08 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Characterize performance for random list generation"
      ],
      "metadata": {
        "id": "qcuWgkpxV0rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title {vertical-output: true}\n",
        "\n",
        "# algorithm specification:\n",
        "# generate an evenly distributed random list that sums to total_sum, minimum value of any element = 1\n",
        "\n",
        "# this will be used extensively during padding, so it's worth spending time optimizing and performance testing\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def verify_random_list(lst, length, total_sum):\n",
        "  if len(lst) != length:\n",
        "    raise ValueError(\"Incorrect length\")\n",
        "  elif sum(lst) != total_sum:\n",
        "    raise ValueError(\"Incorrect sum\")\n",
        "  elif min(lst) < 1:\n",
        "    raise ValueError(\"Minimum value not 1\")\n",
        "\n",
        "\n",
        "def random_list_simple(length, total_sum):\n",
        "  list = [1]*length\n",
        "\n",
        "  # Distribute the total_sum across the list\n",
        "  for i in range(total_sum - length):\n",
        "    list[random.randint(0, length - 1)] += 1\n",
        "\n",
        "  return list\n",
        "\n",
        "\n",
        "def random_list_dist_fill_float(length, total_sum, fill_float):\n",
        "  # Pre-allocate the list to the target length\n",
        "  list = [0]*length\n",
        "\n",
        "  # Range for random value generation for each element\n",
        "  upper_bound = int(fill_float * ((total_sum - length) / length))\n",
        "\n",
        "  if upper_bound < 1:\n",
        "    return random_list_simple(length, total_sum)\n",
        "\n",
        "  # Generate initial list and calculate the current sum\n",
        "  current_sum = 0\n",
        "  for i in range(length):\n",
        "    list[i] = random.randint(1, upper_bound)\n",
        "    current_sum += list[i]\n",
        "\n",
        "  # If current_sum already exceed total_sum, retry the function\n",
        "  if current_sum > total_sum:\n",
        "    return random_list_dist_fill_float(length, total_sum, fill_float)\n",
        "\n",
        "  # Distribute the remaining sum across the list\n",
        "  for i in range(current_sum, total_sum):\n",
        "    list[random.randint(0, length - 1)] += 1\n",
        "\n",
        "  return list\n",
        "\n",
        "\n",
        "def random_list_try_dist_then_simple(length, total_sum, fill_float):\n",
        "  # Pre-allocate the list to the target length\n",
        "  list = [0]*length\n",
        "\n",
        "  # Range for random value generation for each element\n",
        "  upper_bound = int(fill_float * ((total_sum - length) / length))\n",
        "\n",
        "  if upper_bound < 1:\n",
        "    return random_list_simple(length, total_sum)\n",
        "\n",
        "  # Generate initial list and calculate the current sum\n",
        "  current_sum = 0\n",
        "  for i in range(length):\n",
        "    list[i] = random.randint(1, upper_bound)\n",
        "    current_sum += list[i]\n",
        "\n",
        "  # If current_sum already exceed total_sum, retry the function\n",
        "  if current_sum > total_sum:\n",
        "    return random_list_simple(length, total_sum)\n",
        "\n",
        "  # Distribute the remaining sum across the list\n",
        "  for i in range(current_sum, total_sum):\n",
        "    list[random.randint(0, length - 1)] += 1\n",
        "\n",
        "  return list\n",
        "\n",
        "\n",
        "def random_list_np_multinomal(length, total_sum):\n",
        "  adjusted_sum = total_sum - length\n",
        "  result = np.random.multinomial(adjusted_sum, np.ones(length)/length) + 1\n",
        "\n",
        "  return result.tolist()\n",
        "\n",
        "\n",
        "scenarios = {\n",
        "  \"Small Scale 10x\": (100, 1000),\n",
        "  \"Large Scale 5x\": (4000, 21000),\n",
        "  \"Large Scale 2x\": (10500, 21000),\n",
        "  \"Large Scale 1.3x\": (16000, 21000),\n",
        "}\n",
        "\n",
        "functions = [\n",
        "  random_list_simple,\n",
        "  random_list_np_multinomal\n",
        "]\n",
        "\n",
        "float_functions = [\n",
        "  random_list_dist_fill_float,\n",
        "  random_list_try_dist_then_simple,\n",
        "]\n",
        "\n",
        "num_iterations = 100\n",
        "\n",
        "float_values = [1.6, 1.8, 1.95, 2.0]\n",
        "\n",
        "for scenario_name, scenario_values in scenarios.items():\n",
        "  print(f\"Scenario: {scenario_name} (repeated {num_iterations} iterations)\")\n",
        "  for func in functions:\n",
        "    start_time = time.time()\n",
        "\n",
        "    results = []\n",
        "    errors = set()\n",
        "    for _ in range(num_iterations):\n",
        "      try:\n",
        "        res = func(*scenario_values)\n",
        "        results.append(res)\n",
        "        verify_random_list(res, *scenario_values)\n",
        "      except ValueError as e:\n",
        "        errors.add(str(e))\n",
        "\n",
        "    if not errors:\n",
        "      results_df = pd.DataFrame(results)\n",
        "      std_dev = results_df.std()\n",
        "      avg_std_dev = np.mean(std_dev)\n",
        "      max_value = results_df.max().max()\n",
        "      min_value = results_df.min().min()\n",
        "      times_max_occurred = results_df.eq(max_value).sum().sum()\n",
        "      times_min_occurred = results_df.eq(min_value).sum().sum()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    error_message = 'Failed' if errors else 'Succeeded'\n",
        "\n",
        "    print(f\"\\n{func.__name__} ran in {elapsed_time:.2f} s. {error_message}.\")\n",
        "    if errors:\n",
        "      print(f\"Errors: {', '.join(errors)}\")\n",
        "    else:\n",
        "      print(f\"Average standard deviation across iterations: {avg_std_dev:.2f}\")\n",
        "      print(f\"Max value: {max_value} (occurred {times_max_occurred} times)\")\n",
        "      print(f\"Min value: {min_value} (occurred {times_min_occurred} times)\")\n",
        "\n",
        "  # try float functions with different float values\n",
        "  for float_func in float_functions:\n",
        "    for float_value in float_values:\n",
        "      print()\n",
        "      start_time = time.time()\n",
        "\n",
        "      results = []\n",
        "      errors = set()\n",
        "      for _ in range(num_iterations):\n",
        "        try:\n",
        "          res = float_func(*scenario_values, float_value)\n",
        "          results.append(res)\n",
        "          verify_random_list(res, *scenario_values)\n",
        "        except ValueError as e:\n",
        "          errors.add(str(e))\n",
        "\n",
        "      if not errors:\n",
        "        results_df = pd.DataFrame(results)\n",
        "        std_dev = results_df.std()\n",
        "        avg_std_dev = np.mean(std_dev)\n",
        "        max_value = results_df.max().max()\n",
        "        min_value = results_df.min().min()\n",
        "        times_max_occurred = results_df.eq(max_value).sum().sum()\n",
        "        times_min_occurred = results_df.eq(min_value).sum().sum()\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      error_message = 'Failed' if errors else 'Succeeded'\n",
        "\n",
        "      print(f\"{float_func.__name__} (float {float_value}) ran in {elapsed_time:.2f} s. {error_message}.\")\n",
        "      if errors:\n",
        "        print(f\"{', '.join(errors)}\")\n",
        "      else:\n",
        "        print(f\"Average standard deviation across iterations: {avg_std_dev:.2f}\")\n",
        "        print(f\"Max value: {max_value} (occurred {times_max_occurred} times)\")\n",
        "        print(f\"Min value: {min_value} (occurred {times_min_occurred} times)\")\n",
        "\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "Y1o1MDGcR2eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset padding"
      ],
      "metadata": {
        "id": "A2zfzAYAiZlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TODO REMOVE - Generate random variations of each model\n",
        "\n",
        "variations_to_generate = 5\n",
        "\n",
        "# note: this doesn't work properly. remove_overlapping() doesn't revert/fix the artifacting\n",
        "\n",
        "# sorting averages adding 0.2 seconds 1.4 -> 1.6\n",
        "# 1.6 seconds per variation per object:\n",
        "# for 130 models, 58 hours for 1000 variations each\n",
        "\n",
        "\n",
        "for obj in ply_objs:\n",
        "  initial_vertex_count = len(obj.vertices)\n",
        "  initial_face_count = len(obj.faces)\n",
        "  print(f\"Object: {obj.name}\\n\")\n",
        "  print(f'Vertex count: {len(obj.vertices)}')\n",
        "  print(f'Face count: {len(obj.faces)}')\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  obj.export_random_to_model(variations_to_generate, max_vertices, max_faces)\n",
        "  #obj.pad_with_random(max_vertices, max_faces)\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "  print(f'\\nAdded vertices: {len(obj.vertices) - initial_vertex_count}')\n",
        "  print(f'Added faces: {len(obj.faces) - initial_face_count}')\n",
        "  print(f'\\nGenerated {variations_to_generate} random variation(s) (took {elapsed_time:.2f} s)')\n",
        "\n",
        "  print(f'\\nVertex count: {len(obj.vertices)}')\n",
        "  print(f'Face count: {len(obj.faces)}')\n",
        "  print('-' * 50)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q60dVQGvP4QL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2d147a-f51b-4c38-d949-533c9f76d3d7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Vertex count: 16496\n",
            "Face count: 29689\n",
            "\n",
            "Added vertices: 3299\n",
            "Added faces: 5937\n",
            "\n",
            "Generated 5 random variation(s) (took 9.48 s)\n",
            "\n",
            "Vertex count: 19795\n",
            "Face count: 35626\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Vertex count: 12219\n",
            "Face count: 24308\n",
            "\n",
            "Added vertices: 7576\n",
            "Added faces: 11318\n",
            "\n",
            "Generated 5 random variation(s) (took 9.05 s)\n",
            "\n",
            "Vertex count: 19795\n",
            "Face count: 35626\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Vertex count: 7472\n",
            "Face count: 28193\n",
            "\n",
            "Added vertices: 12323\n",
            "Added faces: 7433\n",
            "\n",
            "Generated 5 random variation(s) (took 8.68 s)\n",
            "\n",
            "Vertex count: 19795\n",
            "Face count: 35626\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Vertex count: 12590\n",
            "Face count: 25397\n",
            "\n",
            "Added vertices: 7205\n",
            "Added faces: 10229\n",
            "\n",
            "Generated 5 random variation(s) (took 8.22 s)\n",
            "\n",
            "Vertex count: 19795\n",
            "Face count: 35626\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Vertex count: 7807\n",
            "Face count: 14069\n",
            "\n",
            "Added vertices: 11988\n",
            "Added faces: 21557\n",
            "\n",
            "Generated 5 random variation(s) (took 7.80 s)\n",
            "\n",
            "Vertex count: 19795\n",
            "Face count: 35626\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder initialization"
      ],
      "metadata": {
        "id": "pv5jlq94ermA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define imports and constants\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "# todo: final model should be 1000+\n",
        "variations_to_generate = 50"
      ],
      "metadata": {
        "id": "9jTj6kxEpIRY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import pre-normalized PLY file artifacts (to allow for runtime restart) {vertical-output: true}\n",
        "\n",
        "# pulling from google drive\n",
        "\n",
        "import os\n",
        "\n",
        "ply_objs=[]\n",
        "\n",
        "use_partial_dataset = True\n",
        "\n",
        "for f in os.listdir(DATASET + 'normalized_ply/'):\n",
        "  if f.endswith('.ply') and ((not use_partial_dataset) or f.startswith('Blind Per')):\n",
        "    start_time = time.time()\n",
        "    obj = PlyObject.from_file(filepath=os.path.join(DATASET + 'normalized_ply/', f))\n",
        "    ply_objs.append(obj)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Object: {obj.name}\\n\")\n",
        "    print(f'Processed pre-normalized PLY file into PlyObject (took {elapsed_time:.2f} s)')\n",
        "    print(f'\\nVertice count: {len(obj.vertices)}')\n",
        "    print(f'Face count: {len(obj.faces)}')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "mP9vVnu5JaRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775f79b8-c757-4dca-d0ae-df1db8c1420e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Blind Perdition\n",
            "\n",
            "Processed pre-normalized PLY file into PlyObject (took 0.40 s)\n",
            "\n",
            "Vertice count: 7807\n",
            "Face count: 14069\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Analyze object tri data to determine global target padding size and autoencoder input layer size\n",
        "\n",
        "max_vertices = 0\n",
        "max_faces = 0\n",
        "\n",
        "for obj in ply_objs:\n",
        "  max_vertices = max(max_vertices, len(obj.vertices))\n",
        "  max_faces = max(max_faces, len(obj.faces))\n",
        "\n",
        "print(f'Maximum vertices for any object: {max_vertices}')\n",
        "print(f'Maximum faces for any object: {max_faces}')\n",
        "\n",
        "vertex_target_count = int(max_vertices * 1.2)\n",
        "face_target_count = int(max_faces * 1.2)\n",
        "\n",
        "# make max_faces even so we can encode 2 faces onto eachother\n",
        "face_target_count = face_target_count + face_target_count % 2\n",
        "\n",
        "print(f'\\n120% of vertex count: {vertex_target_count}')\n",
        "print(f'120% of face count: {face_target_count}')\n",
        "\n",
        "vertex_input_size = vertex_target_count\n",
        "face_input_size = face_target_count // 2\n",
        "\n",
        "\n",
        "print(f'\\nVertex input layer width: {vertex_input_size}')\n",
        "print(f'Face input layer width (2 faces per): {face_input_size}')\n",
        "\n",
        "print(f'\\nVertex neurons (3 per): {vertex_input_size * 3}')\n",
        "print(f'Face neurons (3 per): {face_input_size * 3}')\n",
        "print(f'Total Input Neurons: {vertex_input_size * 3 + face_input_size * 3}\\n')"
      ],
      "metadata": {
        "id": "os84buC2qlnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac00072-6eca-47c1-9eb2-42823f7a5eea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum vertices for any object: 7807\n",
            "Maximum faces for any object: 14069\n",
            "\n",
            "120% of vertex count: 9368\n",
            "120% of face count: 16882\n",
            "\n",
            "Vertex input layer width: 9368\n",
            "Face input layer width (2 faces per): 8441\n",
            "\n",
            "Vertex neurons (3 per): 28104\n",
            "Face neurons (3 per): 25323\n",
            "Total Input Neurons: 53427\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define variational sampling function\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.random.normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "F-TfRVNSuaas"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize encoder model\n",
        "\n",
        "vertex_layer_size = 512\n",
        "face_layer_size = 256\n",
        "\n",
        "# todo: final model should be 64\n",
        "combination_layer_1_size = 16\n",
        "\n",
        "combination_layer_2_size = 16384\n",
        "latent_dim = 32\n",
        "\n",
        "vertex_inputs = keras.Input(shape=(vertex_input_size, 3))\n",
        "face_inputs = keras.Input(shape=(face_input_size, 3))\n",
        "\n",
        "x = layers.Dense(vertex_layer_size, activation=\"relu\")(vertex_inputs)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "y = layers.Dense(face_layer_size, activation=\"relu\")(face_inputs)\n",
        "y = layers.Flatten()(y)\n",
        "\n",
        "x = layers.Concatenate()([x, y])\n",
        "\n",
        "x = layers.Dense(combination_layer_1_size, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(combination_layer_2_size, activation=\"relu\")(x)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "encoder = keras.Model((vertex_inputs, face_inputs), (z_mean, z_log_var, z), name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "x1I-hJ5cwatz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880ecd27-1e2d-4bf5-a614-9b16ef76607a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 9368, 3)]    0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 8441, 3)]    0           []                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 9368, 512)    2048        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 8441, 256)    1024        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 4796416)      0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 2160896)      0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 6957312)      0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 16)           111317008   ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 16)           0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16384)        278528      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 32)           524320      ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 32)           524320      ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 32)           0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 112,647,248\n",
            "Trainable params: 112,647,248\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize Decoder Model\n",
        "\n",
        "# todo: final model should be 3+\n",
        "dense_layer_multiplier = 0.2\n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(combination_layer_2_size, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Dense(combination_layer_1_size, activation=\"relu\")(x)\n",
        "x = layers.Dense(int(vertex_layer_size * face_layer_size * dense_layer_multiplier), activation=\"relu\")(x)\n",
        "\n",
        "x1 = layers.Dense(vertex_layer_size * 3, activation=\"relu\")(x)\n",
        "x1 = layers.Dense(vertex_input_size * 3, activation=\"relu\")(x1)\n",
        "x1 = layers.Reshape((vertex_input_size, 3))(x1)\n",
        "\n",
        "x2 = layers.Dense(face_layer_size * 3, activation=\"relu\")(x)\n",
        "x2 = layers.Dense(face_input_size * 3, activation=\"relu\")(x2)\n",
        "x2 = layers.Reshape((face_input_size, 3))(x2)\n",
        "\n",
        "decoder = keras.Model(latent_inputs, (x1, x2), name=\"decoder\")\n",
        "decoder.summary()\n"
      ],
      "metadata": {
        "id": "62Bsfx3kyH9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac06122-5482-4168-a863-583cb22f8200"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 32)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 16384)        540672      ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 16)           262160      ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 26214)        445638      ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 1536)         40266240    ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 768)          20133120    ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 28104)        43195848    ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 25323)        19473387    ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 9368, 3)      0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 8441, 3)      0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,317,065\n",
            "Trainable params: 124,317,065\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define VAE class\n",
        "\n",
        "class VAE(keras.Model):\n",
        "  def __init__(self, encoder, decoder, beta=0.1, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.beta = beta\n",
        "    self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "    self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "      name=\"reconstruction_loss\"\n",
        "    )\n",
        "    self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [\n",
        "      self.total_loss_tracker,\n",
        "      self.reconstruction_loss_tracker,\n",
        "      self.kl_loss_tracker,\n",
        "    ]\n",
        "\n",
        "  def train_step(self, data):\n",
        "    with tf.GradientTape() as tape:\n",
        "      z_mean, z_log_var, z = self.encoder(data)\n",
        "      reconstruction = self.decoder(z)\n",
        "      # These are the splitted data and its reconstruction\n",
        "      data_vertex, data_face = data[0]\n",
        "      reconstruction_vertex, reconstruction_face = reconstruction\n",
        "\n",
        "      reconstruction_loss_vertex = tf.reduce_mean(\n",
        "        tf.reduce_sum(\n",
        "          keras.losses.mean_squared_error(data_vertex, reconstruction_vertex), axis=(1)\n",
        "        )\n",
        "      )\n",
        "\n",
        "      reconstruction_loss_face = tf.reduce_mean(\n",
        "        tf.reduce_sum(\n",
        "          keras.losses.mean_squared_error(data_face, reconstruction_face), axis=(1)\n",
        "        )\n",
        "      )\n",
        "\n",
        "      reconstruction_loss = reconstruction_loss_vertex + reconstruction_loss_face\n",
        "\n",
        "      kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "      kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "      total_loss = reconstruction_loss + self.beta * kl_loss\n",
        "    grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "    self.total_loss_tracker.update_state(total_loss)\n",
        "    self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "    self.kl_loss_tracker.update_state(kl_loss)\n",
        "    return {\n",
        "      \"loss\": self.total_loss_tracker.result(),\n",
        "      \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "      \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "    }"
      ],
      "metadata": {
        "id": "OYxktNPW1ivM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Randomly Pad and Convert PlyObjects to input arrays {vertical-output: true}\n",
        "\n",
        "# Doing this step before defining encoder and decoder to avoid system memory cap\n",
        "\n",
        "# batch testing performance results:\n",
        "# 900 variations, 5 objs\n",
        "# 7783 seconds\n",
        "\n",
        "dataset_size = len(ply_objs) * variations_to_generate\n",
        "\n",
        "vertex_input_list = np.zeros((dataset_size, vertex_input_size, 3))\n",
        "face_input_list = np.zeros((dataset_size, face_input_size, 3))\n",
        "\n",
        "idx = 0\n",
        "for obj in ply_objs:\n",
        "  print(f\"Object : {obj.name}\\n\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  variations = obj.export_random_to_model(variations_to_generate, vertex_target_count, face_target_count)\n",
        "  variation_vertices = variations['vertices']\n",
        "  variation_faces = variations['faces']\n",
        "  for i in range(len(variation_vertices)):\n",
        "    vertex_input_list[idx] = variation_vertices[i]\n",
        "    face_input_list[idx] = variation_faces[i]\n",
        "    idx += 1\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "  print(f'Generated {variations_to_generate} random variation(s) (took {elapsed_time:.2f} s)')\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO6fkfz5gLkA",
        "outputId": "fd943615-b376-48a9-a2ef-9076a4c2be4d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object : Blind Perdition\n",
            "\n",
            "Generated 50 random variation(s) (took 27.92 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vertex_input_list.shape)\n",
        "print(face_input_list.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpzYauT5lCPe",
        "outputId": "97904ca6-7377-46f8-887b-f616e3755d13"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 9368, 3)\n",
            "(50, 8441, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train models using defined VAE\n",
        "\n",
        "# estimating model size = ((450+720) = mil params * 32 bits) = 4.7GB\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "\n",
        "vae.fit([vertex_input_list, face_input_list], epochs=30, batch_size=dataset_size)"
      ],
      "metadata": {
        "id": "uVWkGYWH4bgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9db4e9a-f54b-41e8-a073-9d9e157d2414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1/1 [==============================] - 13s 13s/step - loss: nan - reconstruction_loss: nan - kl_loss: inf\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 6s 6s/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 7s 7s/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 5s 5s/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 7s 7s/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 5s 5s/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 7s 7s/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 5s 5s/step - loss: nan - reconstruction_loss: nan - kl_loss: nan\n",
            "Epoch 9/30\n"
          ]
        }
      ]
    }
  ]
}