{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWs+dqQCsc0M9JX8dkV3ft",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackschedel/AutoCalibr/blob/main/AutoCalibr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset importing"
      ],
      "metadata": {
        "id": "gKVZUbqheVJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define imports and constants\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "DRIVE = '/content/drive/MyDrive/AutoCalibr/'\n",
        "DATASET = DRIVE + 'dataset/'\n",
        "\n",
        "# intermediate folders\n",
        "INTERMEDIATES = '/content/intermediates'\n",
        "CONVERTED_PLY = INTERMEDIATES + '/converted_ply/'\n",
        "NORMALIZED_PLY = INTERMEDIATES + '/normalized_ply/'\n",
        "\n",
        "# sometimes used for debug outputting into non-cluttered directory\n",
        "DIR = '/content/'\n",
        "\n",
        "!rm -r sample_data/ 2>/dev/null\n",
        "!mkdir {INTERMEDIATES} 2>/dev/null"
      ],
      "metadata": {
        "id": "SlrX3zBJ9YYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90559833-f9fd-4136-f9b3-97ec3a9680dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=2d36a3640a34287107bb1cbf7f93bf880af520d10442fd0b7d208c1b3a1fc5ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.8\n",
            "--------------------------------------------------\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert FBX to PLY {vertical-output: true}\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import bpy\n",
        "except ImportError:\n",
        "  !pip install bpy\n",
        "  print('-' * 50)\n",
        "  import bpy\n",
        "\n",
        "!rm -r {CONVERTED_PLY} 2>/dev/null\n",
        "!mkdir {CONVERTED_PLY}\n",
        "\n",
        "bpy.ops.wm.read_factory_settings()\n",
        "\n",
        "# needs to be rotated counterclockwise (left) 90 degrees - could be mostly automated (noted below)\n",
        "needs_extra_rotation = [\"Ace Of Spades\", \"Cantata-57\", \"Cloudstrike\", \"Dead Mans Tale\", \"Duality\", \"False Promises\", \"Fugue 55\", \"Hawkmoon\", \"Jack Queen King 3\", \"Mindbenders Ambition\", \"No Time To Explain\", \"Ruinous Effigy\", \"Seven Seraph Carbine\", \"Seventh Seraph CQC-12\", \"Seventh Seraph Officer Revolver\", \"Seventh Seraph SAW\", \"Seventh Seraph SI-2\", \"Seventh Seraph VY-7\", \"Trustee\", \"Witherhoard\"]\n",
        "\n",
        "fbx_dir = DATASET + 'fbx/'\n",
        "\n",
        "for f in os.listdir(fbx_dir):\n",
        "  if f.endswith('.fbx'):\n",
        "    # Isolate the name of the .fbx file (without extension)\n",
        "    name_no_ext = os.path.splitext(os.path.basename(f))[0]\n",
        "\n",
        "    print(f\"Object: {name_no_ext}\\n\")\n",
        "\n",
        "    # Delete all mesh objects to avoid exporting multiple models into the same file\n",
        "    bpy.ops.object.select_all(action='DESELECT')\n",
        "    bpy.ops.object.select_by_type(type='MESH')\n",
        "    bpy.ops.object.delete()\n",
        "\n",
        "    # Load in FBX file\n",
        "    bpy.ops.import_scene.fbx(filepath=os.path.join(fbx_dir, f))\n",
        "\n",
        "    # Select the object\n",
        "    obj_object = bpy.context.selected_objects[0]\n",
        "    bpy.context.view_layer.objects.active = obj_object\n",
        "\n",
        "    if name_no_ext in needs_extra_rotation:\n",
        "      forwards_dir = 'Z'\n",
        "      needs_extra_rotation.remove(name_no_ext)\n",
        "    else:\n",
        "      forwards_dir = '-X'\n",
        "\n",
        "    # Export object to PLY\n",
        "    bpy.ops.export_mesh.ply(filepath=os.path.join(CONVERTED_PLY, f.replace('.fbx', '.ply')), use_ascii=True, use_mesh_modifiers=True, use_normals=False, use_uv_coords=False, use_colors=False, axis_forward=forwards_dir, axis_up='Y')\n",
        "\n",
        "    print('-' * 50)\n",
        "\n",
        "# Ensure that any models that were supposed to receive extra rotation were hit\n",
        "if len(needs_extra_rotation) > 0:\n",
        "  print(f\"\\nThe following manually-specified models were not hit (check for typos): {needs_extra_rotation}\")"
      ],
      "metadata": {
        "id": "iG6Uc5bGslQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset processing"
      ],
      "metadata": {
        "id": "i79LmUiYMrJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define PlyObject Class\n",
        "import random\n",
        "import math\n",
        "from random import choice\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "def random_list_of_sum(length, total_sum):\n",
        "  adjusted_sum = total_sum - length\n",
        "  result = np.random.multinomial(adjusted_sum, np.ones(length)/length) + 1\n",
        "\n",
        "  return result.tolist()\n",
        "\n",
        "\n",
        "class Vertex:\n",
        "  def __init__(self, x, y, z):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.z = z\n",
        "\n",
        "\n",
        "  def copy(self):\n",
        "    return Vertex(self.x, self.y, self.z)\n",
        "\n",
        "\n",
        "  def distance(self, other_vertex):\n",
        "    diff_x = self.x - other_vertex.x\n",
        "    diff_y = self.y - other_vertex.y\n",
        "    diff_z = self.z - other_vertex.z\n",
        "\n",
        "    distance = math.sqrt(diff_x**2 + diff_y**2 + diff_z**2)\n",
        "    return distance\n",
        "\n",
        "\n",
        "  def to_list(self):\n",
        "    return [self.x, self.y, self.z]\n",
        "\n",
        "\n",
        "  def scale(self, scale_x, scale_y, scale_z):\n",
        "    self.x *= scale_x\n",
        "    self.y *= scale_y\n",
        "    self.z *= scale_z\n",
        "\n",
        "\n",
        "  def translate(self, offset_x, offset_y, offset_z):\n",
        "    self.x += offset_x\n",
        "    self.y += offset_y\n",
        "    self.z += offset_z\n",
        "\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash((self.x, self.y, self.z))\n",
        "\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    if isinstance(other, Vertex):\n",
        "      return self.x == other.x and self.y == other.y and self.z == other.z\n",
        "    return False\n",
        "\n",
        "\n",
        "class Face:\n",
        "  def __init__(self, vertices):\n",
        "    # vertices is a list of indexes (of the object's vertices list) of the connected vertices that form the face\n",
        "    self.vertices = vertices\n",
        "\n",
        "\n",
        "  def copy(self):\n",
        "    return Face(self.vertices.copy())\n",
        "\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    for val1, val2 in zip(self.vertices, other.vertices):\n",
        "      if val1 < val2:\n",
        "        return True\n",
        "      elif val1 > val2:\n",
        "        return False\n",
        "    return len(self.vertices) < len(other.vertices)\n",
        "\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(tuple(self.vertices))\n",
        "\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    if isinstance(other, Face):\n",
        "      return (sorted(self.vertices) == sorted(other.vertices))\n",
        "    return False\n",
        "\n",
        "\n",
        "class PlyObject:\n",
        "  def __init__(self, name, vertices, faces, id):\n",
        "    self.name = name\n",
        "    self.vertices = vertices\n",
        "    self.faces = faces\n",
        "    # lazy me doesnt want to refactor every processing loop so doing this for printing\n",
        "    self.id = id\n",
        "\n",
        "\n",
        "  def copy(self):\n",
        "    return PlyObject(self.name, [v.copy() for v in self.vertices], [f.copy() for f in self.faces], self.id)\n",
        "\n",
        "\n",
        "  def get_2d_face_list(self):\n",
        "    return np.array([face.vertices for face in self.faces])\n",
        "\n",
        "\n",
        "  def get_2d_encoded_face_list(self):\n",
        "    # Create an empty 2D numpy array with the correct shape\n",
        "    encoded_faces = np.empty((len(self.faces) // 2, 3), dtype=np.int32)\n",
        "\n",
        "    # Process two faces at a time\n",
        "    for i in range(0, len(self.faces), 2):\n",
        "      face1 = self.faces[i]\n",
        "      face2 = self.faces[i + 1]\n",
        "\n",
        "      # Process each property (x, y, z)\n",
        "      for j in range(3):\n",
        "        # Cast to 16-bit, then encode together as a 32-bit value using alternating digits\n",
        "        value1_16 = np.int16(face1.vertices[j])\n",
        "        value2_16 = np.int16(face2.vertices[j])\n",
        "        encoded = np.bitwise_or.reduce(np.bitwise_or(np.left_shift(value1_16, np.arange(16)*2), np.left_shift(value2_16, np.arange(16)*2 + 1)))\n",
        "\n",
        "        # Store in the result array\n",
        "        encoded_faces[i // 2, j] = encoded\n",
        "\n",
        "    return encoded_faces\n",
        "\n",
        "\n",
        "  def get_2d_vertex_list(self):\n",
        "    return np.array([[vertex.x, vertex.y, vertex.z] for vertex in self.vertices])\n",
        "\n",
        "\n",
        "  def pad_with_random(self, max_vertices, max_faces):\n",
        "    self.subdivide_faces_as_padding(max_vertices, max_faces)\n",
        "\n",
        "    vertices_to_add = max_vertices - len(self.vertices)\n",
        "    faces_to_add = max_faces - len(self.faces)\n",
        "\n",
        "    if vertices_to_add > 0:\n",
        "      self.add_random_duplicate_vertices(vertices_to_add)\n",
        "    if faces_to_add > 0:\n",
        "      self.add_random_duplicate_faces(faces_to_add)\n",
        "\n",
        "    self.sort_tri_data(True)\n",
        "\n",
        "\n",
        "  def export_random_to_model(self, count, max_vertices, max_faces):\n",
        "    vertex_list_3d = np.zeros((count, max_vertices, 3))\n",
        "    face_list_3d = np.zeros((count, max_faces, 3))\n",
        "\n",
        "    for i in range(count):\n",
        "      new_variation = self\n",
        "\n",
        "      original_vertices = self.vertices.copy()\n",
        "      original_faces = self.faces.copy()\n",
        "\n",
        "      self.pad_with_random(max_vertices, max_faces)\n",
        "\n",
        "      vertex_list_3d[i] = self.get_2d_vertex_list()\n",
        "      face_list_3d[i] = self.get_2d_face_list()\n",
        "\n",
        "      self.vertices = original_vertices\n",
        "      self.faces = original_faces\n",
        "\n",
        "    return { 'vertices': vertex_list_3d, 'faces': face_list_3d }\n",
        "\n",
        "\n",
        "  def save_file(self, filename):\n",
        "    with open(filename, \"w\") as file:\n",
        "      file.write(\"ply\\n\")\n",
        "      file.write(\"format ascii 1.0\\n\")\n",
        "      file.write(\"comment Created by PlyObject class\\n\")\n",
        "      file.write(f\"element vertex {len(self.vertices)}\\n\")\n",
        "      file.write(\"property float x\\n\")\n",
        "      file.write(\"property float y\\n\")\n",
        "      file.write(\"property float z\\n\")\n",
        "      file.write(f\"element face {len(self.faces)}\\n\")\n",
        "      file.write(\"property list uchar uint vertex_indices\\n\")\n",
        "      file.write(\"end_header\\n\")\n",
        "\n",
        "      for vertex in self.vertices:\n",
        "        file.write(f\"{vertex.x} {vertex.y} {vertex.z}\\n\")\n",
        "\n",
        "      for face in self.faces:\n",
        "        formatted_vertices = ' '.join(str(v) for v in face.vertices)\n",
        "        file.write(f\"{len(face.vertices)} {formatted_vertices}\\n\")\n",
        "\n",
        "\n",
        "  def scale(self, scale_x, scale_y, scale_z):\n",
        "    for vertex in self.vertices:\n",
        "      vertex.scale(scale_x, scale_y, scale_z)\n",
        "\n",
        "\n",
        "  def translate(self, offset_x, offset_y, offset_z):\n",
        "    for vertex in self.vertices:\n",
        "      vertex.translate(offset_x, offset_y, offset_z)\n",
        "\n",
        "\n",
        "  def calculate_volume(self):\n",
        "    volume = 0\n",
        "    for face in self.faces:\n",
        "      v0 = self.vertices[face.vertices[0]]\n",
        "      v1 = self.vertices[face.vertices[1]]\n",
        "      v2 = self.vertices[face.vertices[2]]\n",
        "      volume += (-v0.x*v1.y*v2.z + v1.x*v0.y*v2.z + v0.x*v2.y*v1.z - v2.x*v0.y*v1.z + v2.x*v1.y*v0.z - v1.x*v2.y*v0.z)\n",
        "    return abs(volume) / 6.0\n",
        "\n",
        "\n",
        "  def remove_overlapping(self):\n",
        "    vert_dict = {}\n",
        "    convert_dict = {}\n",
        "\n",
        "    new_vertices = []\n",
        "    # iterate over existing vertices to identify and save unique ones\n",
        "    for idx, vertex in enumerate(self.vertices):\n",
        "      if vertex in vert_dict:\n",
        "        convert_dict[idx] = vert_dict[vertex]\n",
        "      else:\n",
        "        # assign a unique index to each vertex\n",
        "        vert_dict[vertex] = len(new_vertices)\n",
        "        convert_dict[idx] = len(new_vertices)\n",
        "        new_vertices.append(vertex)\n",
        "\n",
        "    # replace original vertices with new, duplicate-free list\n",
        "    self.vertices = new_vertices\n",
        "\n",
        "    # apply convert_dict to update face vertices to match new unique indexing\n",
        "    for face in self.faces:\n",
        "      face.vertices = [convert_dict[vertex] for vertex in face.vertices]\n",
        "\n",
        "    # convert faces to set to remove any potential duplicate faces\n",
        "    self.faces = set(self.faces)\n",
        "    self.faces = list(set(self.faces))\n",
        "\n",
        "\n",
        "  # not great for scale since re-sorting is needed, use a batch version after dataset processing\n",
        "  def add_random_duplicate_vertices(self, count_to_add):\n",
        "    # note: will need to re-sort tri data if already sorted\n",
        "    for _ in range(count_to_add):\n",
        "      to_duplicate = choice(self.vertices)\n",
        "      new_vertex = Vertex(to_duplicate.x, to_duplicate.y, to_duplicate.z)\n",
        "      self.vertices.append(new_vertex)\n",
        "\n",
        "\n",
        "  # not great for scale since re-sorting is needed, use a batch version after dataset processing\n",
        "  def add_random_duplicate_faces(self, count_to_add):\n",
        "    # note: will need to re-sort tri data if already sorted\n",
        "    for _ in range(count_to_add):\n",
        "      to_duplicate = choice(self.faces)\n",
        "      new_face = Face(to_duplicate.vertices)\n",
        "      self.faces.append(new_face)\n",
        "\n",
        "\n",
        "  def subdivide_face(self, face_index):\n",
        "    # Get the face to be subdivided\n",
        "    face = self.faces[face_index]\n",
        "\n",
        "    # Find vertices to split between and create new vertex in between\n",
        "    vertex_index_1, vertex_index_2 = face.vertices[0], face.vertices[1]\n",
        "    vertex_1, vertex_2 = self.vertices[vertex_index_1], self.vertices[vertex_index_2]\n",
        "\n",
        "    new_vertex = Vertex((vertex_1.x + vertex_2.x) / 2, (vertex_1.y + vertex_2.y) / 2, (vertex_1.z + vertex_2.z) / 2)\n",
        "\n",
        "    # Add new_vertex to the vertices list and store its index\n",
        "    self.vertices.append(new_vertex)\n",
        "    new_vertex_index = len(self.vertices) - 1\n",
        "\n",
        "    # Create two new faces with correct order to maintain outward normal\n",
        "    new_face_1 = Face([vertex_index_1, new_vertex_index, face.vertices[2]])\n",
        "    new_face_2 = Face([new_vertex_index, vertex_index_2, face.vertices[2]])\n",
        "\n",
        "    # Replace the old face with the new faces\n",
        "    self.faces[face_index] = new_face_1\n",
        "    self.faces.append(new_face_2)\n",
        "\n",
        "\n",
        "\n",
        "  def subdivide_faces_as_padding(self, max_vertices, max_faces):\n",
        "    # note: loses sorted status\n",
        "    while len(self.faces) < max_faces and len(self.vertices) < max_vertices:\n",
        "      face_index = random.randint(0, len(self.faces) - 1)\n",
        "      obj.subdivide_face(face_index)\n",
        "\n",
        "\n",
        "  def categorize_faces(self):\n",
        "    face_dict = {}\n",
        "\n",
        "    for face in obj.faces:\n",
        "      length = len(face.vertices)\n",
        "\n",
        "      if length in face_dict:\n",
        "        face_dict[length] = face_dict[length] + 1\n",
        "      else:\n",
        "        face_dict[length] = 1\n",
        "\n",
        "    return face_dict\n",
        "\n",
        "\n",
        "  def get_value_extrema(self):\n",
        "    min_val = float('inf')\n",
        "    max_val = float('-inf')\n",
        "\n",
        "    for v in self.vertices:\n",
        "      min_val = min(min_val, v.x, v.y, v.z)\n",
        "      max_val = max(max_val, v.x, v.y, v.z)\n",
        "\n",
        "    return {'min': min_val, 'max': max_val}\n",
        "\n",
        "\n",
        "  def get_max_values(self):\n",
        "    max_values = {'x': None, 'y': None, 'z': None}\n",
        "\n",
        "    for vertex in self.vertices:\n",
        "      if max_values['x'] is None or vertex.x > max_values['x']:\n",
        "        max_values['x'] = vertex.x\n",
        "      if max_values['y'] is None or vertex.y > max_values['y']:\n",
        "        max_values['y'] = vertex.y\n",
        "      if max_values['z'] is None or vertex.z > max_values['z']:\n",
        "        max_values['z'] = vertex.z\n",
        "\n",
        "    return max_values\n",
        "\n",
        "\n",
        "  def get_min_values(self):\n",
        "    min_values = {'x': None, 'y': None, 'z': None}\n",
        "\n",
        "    for vertex in self.vertices:\n",
        "      if min_values['x'] is None or vertex.x < min_values['x']:\n",
        "        min_values['x'] = vertex.x\n",
        "      if min_values['y'] is None or vertex.y < min_values['y']:\n",
        "        min_values['y'] = vertex.y\n",
        "      if min_values['z'] is None or vertex.z < min_values['z']:\n",
        "        min_values['z'] = vertex.z\n",
        "\n",
        "    return min_values\n",
        "\n",
        "\n",
        "  def center_object(self):\n",
        "    max_vals = self.get_max_values()\n",
        "    min_vals = self.get_min_values()\n",
        "\n",
        "    offset_vals = {'x': 0, 'y': 0, 'z': 0}\n",
        "\n",
        "    for dim in offset_vals:\n",
        "      center = (max_vals[dim] + min_vals[dim]) / 2\n",
        "      offset_vals[dim] = -center\n",
        "\n",
        "    self.translate(offset_vals['x'], offset_vals['y'], offset_vals['z'])\n",
        "\n",
        "\n",
        "  def normalize_scale(self):\n",
        "    x_coordinates = [vertex.x for vertex in self.vertices]\n",
        "    y_coordinates = [vertex.y for vertex in self.vertices]\n",
        "    z_coordinates = [vertex.z for vertex in self.vertices]\n",
        "\n",
        "    max_distance = max(x_coordinates + y_coordinates + z_coordinates)\n",
        "    min_distance = min(x_coordinates + y_coordinates + z_coordinates)\n",
        "    normalization_range = max_distance - min_distance\n",
        "\n",
        "    if normalization_range == 0:\n",
        "      raise ValueError(\"Normalization range cannot be zero\")\n",
        "\n",
        "    for vertex in self.vertices:\n",
        "      vertex.x = 2 * (vertex.x - min_distance) / normalization_range - 1\n",
        "      vertex.y = 2 * (vertex.y - min_distance) / normalization_range - 1\n",
        "      vertex.z = 2 * (vertex.z - min_distance) / normalization_range - 1\n",
        "\n",
        "\n",
        "  def squares_to_tris(self):\n",
        "    new_faces = []\n",
        "    for face in self.faces:\n",
        "      if len(face.vertices) == 4:\n",
        "        new_faces.append(Face([face.vertices[0], face.vertices[1], face.vertices[2]]))\n",
        "        new_faces.append(Face([face.vertices[0], face.vertices[2], face.vertices[3]]))\n",
        "      else:\n",
        "        new_faces.append(face)\n",
        "\n",
        "    self.faces = new_faces\n",
        "\n",
        "\n",
        "  def delete_plane(self):\n",
        "    # Note: only call if the object has a plane artifact!\n",
        "    max_x_index = max(range(len(self.vertices)), key = lambda index: self.vertices[index].x)\n",
        "    min_x_index = min(range(len(self.vertices)), key = lambda index: self.vertices[index].x)\n",
        "    max_z_index = max(range(len(self.vertices)), key = lambda index: self.vertices[index].z)\n",
        "    min_z_index = min(range(len(self.vertices)), key = lambda index: self.vertices[index].z)\n",
        "\n",
        "    indices_to_remove = set([max_x_index, min_x_index, max_z_index, min_z_index])\n",
        "\n",
        "    for f, face in enumerate(self.faces):\n",
        "      if set(face.vertices).intersection(indices_to_remove) == indices_to_remove:\n",
        "        del self.faces[f]\n",
        "        break\n",
        "    else:\n",
        "      raise ValueError(\"Face with all vertices not found\")\n",
        "\n",
        "    for index in sorted(indices_to_remove, reverse=True):\n",
        "      del self.vertices[index]\n",
        "\n",
        "    for face in self.faces:\n",
        "      face.vertices = [idx if idx not in indices_to_remove else -1 for idx in face.vertices]\n",
        "\n",
        "    self.remove_disconnected_vertices()\n",
        "\n",
        "\n",
        "  def remove_disconnected_vertices(self):\n",
        "    connected_vertices = set()\n",
        "    for face in self.faces:\n",
        "      connected_vertices |= set(face.vertices)\n",
        "\n",
        "    shift_indices = []\n",
        "    new_vertices = []\n",
        "    for idx, vertex in enumerate(self.vertices):\n",
        "      if idx in connected_vertices:\n",
        "        new_vertices.append(vertex)\n",
        "        shift_indices.append(len(new_vertices) - 1)\n",
        "      else:\n",
        "        shift_indices.append(None)\n",
        "\n",
        "    self.vertices = new_vertices\n",
        "\n",
        "    for face in self.faces:\n",
        "      face.vertices = [shift_indices[vertex] if shift_indices[vertex] is not None else None for vertex in face.vertices]\n",
        "      face.vertices = [vertex for vertex in face.vertices if vertex is not None]\n",
        "\n",
        "    self.faces = [face for face in self.faces if face.vertices]\n",
        "\n",
        "\n",
        "  def stretch_to_max(self):\n",
        "    # note: update scale globals before calling\n",
        "    self.scale(global_stretch_scale_x, global_stretch_scale_y, global_stretch_scale_z)\n",
        "\n",
        "\n",
        "  def revert_stretch(self):\n",
        "    self.scale(1/global_stretch_scale_x, 1/global_stretch_scale_y, 1/global_stretch_scale_z)\n",
        "\n",
        "\n",
        "  def dimension_range(self):\n",
        "    xmin = xmax = obj.vertices[0].x\n",
        "    ymin = ymax = obj.vertices[0].y\n",
        "    zmin = zmax = obj.vertices[0].z\n",
        "\n",
        "    for vertex in obj.vertices:\n",
        "      xmin = min(xmin, vertex.x)\n",
        "      xmax = max(xmax, vertex.x)\n",
        "      ymin = min(ymin, vertex.y)\n",
        "      ymax = max(ymax, vertex.y)\n",
        "      zmin = min(zmin, vertex.z)\n",
        "      zmax = max(zmax, vertex.z)\n",
        "\n",
        "    xrang = xmax - xmin\n",
        "    yrang = ymax - ymin\n",
        "    zrang = zmax - zmin\n",
        "\n",
        "    return {'x':xrang, 'y':yrang, 'z':zrang}\n",
        "\n",
        "\n",
        "  def sort_tri_data(self, break_normals = True):\n",
        "    index_map = {}\n",
        "\n",
        "    # sort the vertices by x first, then y, then z\n",
        "    sorted_vertices = sorted(\n",
        "      enumerate(self.vertices),\n",
        "      key=lambda pair: (pair[1].x, pair[1].y, pair[1].z)\n",
        "    )\n",
        "    self.vertices = [pair[1] for pair in sorted_vertices]\n",
        "\n",
        "    # record the new indices of the vertices in the map\n",
        "    for i, pair in enumerate(sorted_vertices):\n",
        "      old_index, _ = pair\n",
        "      index_map[old_index] = i\n",
        "\n",
        "    # convert old face lists to new face lists using the index map\n",
        "    new_faces = []\n",
        "    for face in self.faces:\n",
        "      new_face_vertices = [index_map[i] for i in face.vertices]\n",
        "      # new_face_vertices.sort()\n",
        "      if break_normals:\n",
        "        new_face_vertices.sort()\n",
        "      new_faces.append(Face(new_face_vertices))\n",
        "\n",
        "    # replace old face list with new face list, which we first sort\n",
        "    new_faces.sort()\n",
        "\n",
        "    self.faces = new_faces\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_file(cls, filepath, index):\n",
        "    with open(filepath, 'r') as f:\n",
        "      if next(f).strip() != \"ply\":\n",
        "        raise ValueError(\"The file being read is not a PLY file.\")\n",
        "\n",
        "      for _ in range(2):\n",
        "        next(f)\n",
        "\n",
        "      n_vertices = int(next(f).split()[-1])\n",
        "\n",
        "      for _ in range(3):\n",
        "        next(f)\n",
        "\n",
        "      n_faces = int(next(f).split()[-1])\n",
        "\n",
        "      for _ in range(2):\n",
        "        next(f)\n",
        "\n",
        "      vertices = []\n",
        "      for _ in range(n_vertices):\n",
        "        x, y, z = map(float, next(f).split())\n",
        "        vertices.append(Vertex(x, y, z))\n",
        "\n",
        "      faces = []\n",
        "      for _ in range(n_faces):\n",
        "        face_vertices = list(map(int, next(f).split()[1:]))\n",
        "        faces.append(Face(face_vertices))\n",
        "\n",
        "    name = os.path.splitext(os.path.basename(filepath))[0]\n",
        "\n",
        "    return cls(name, vertices, faces, index)\n",
        "\n",
        "  @classmethod\n",
        "  def from_model(cls, name, vertices_input, faces_input, index):\n",
        "    vertices_input = vertices_input.numpy()\n",
        "    faces_input = faces_input.numpy()\n",
        "\n",
        "    vertices = []\n",
        "    for vertex in vertices_input:\n",
        "      vertices.append(Vertex(vertex[0], vertex[1], vertex[2]))\n",
        "\n",
        "    faces = []\n",
        "    for face in faces_input:\n",
        "      faces.append(Face([round(face[0]), round(face[1]), round(face[2])]))\n",
        "\n",
        "    return cls(name, vertices, faces, index)"
      ],
      "metadata": {
        "id": "xwXw3NRMICxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Process PLY files as PlyObject {vertical-output: true}\n",
        "\n",
        "import os\n",
        "\n",
        "# Toggle between using full dataset and just preconverted subset\n",
        "use_fbx_dataset = False\n",
        "use_ply_dataset = True\n",
        "# if both are false, it will use a single test file from the ply dataset\n",
        "\n",
        "ply_objs = []\n",
        "\n",
        "idx = 0\n",
        "if use_fbx_dataset:\n",
        "  for f in os.listdir(CONVERTED_PLY):\n",
        "    if f.endswith('.ply'):\n",
        "      start_time = time.time()\n",
        "      obj = PlyObject.from_file(os.path.join(CONVERTED_PLY, f), idx)\n",
        "      ply_objs.append(obj)\n",
        "      elapsed_time = time.time() - start_time\n",
        "\n",
        "      idx += 1\n",
        "      print(f\"Object: {obj.name}\\n\")\n",
        "      print(f'Processed auto-converted PLY into PlyObject (took {elapsed_time:.2f} s)')\n",
        "      print(f'\\nVertice count: {len(obj.vertices)}')\n",
        "      print(f'Face count: {len(obj.faces)}')\n",
        "      print('-' * 50)\n",
        "\n",
        "\n",
        "# Import pre-converted .ply files\n",
        "if use_ply_dataset:\n",
        "  for f in os.listdir(DATASET+'ply/'):\n",
        "    if f.endswith('.ply'):\n",
        "      start_time = time.time()\n",
        "      obj = PlyObject.from_file(os.path.join(DATASET+'ply/', f), idx)\n",
        "      ply_objs.append(obj)\n",
        "      elapsed_time = time.time() - start_time\n",
        "\n",
        "      idx += 1\n",
        "      print(f\"Object: {obj.name}\\n\")\n",
        "      print(f'Processed pre-converted PLY into PlyObject (took {elapsed_time:.2f} s)')\n",
        "      print(f'\\nVertice count: {len(obj.vertices)}')\n",
        "      print(f'Face count: {len(obj.faces)}')\n",
        "      print('-' * 50)\n",
        "\n",
        "if not (use_ply_dataset or use_fbx_dataset):\n",
        "  f = DATASET+'ply/'+'Blind Perdition.ply'\n",
        "  if not os.path.isfile(f):\n",
        "    raise Exception(f'{f} not found!')\n",
        "\n",
        "  start_time = time.time()\n",
        "  obj = PlyObject.from_file(os.path.join(DATASET+'ply/', f), 0)\n",
        "  ply_objs.append(obj)\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  print(f\"Object: {obj.name}\\n\")\n",
        "  print(f'Processed pre-converted PLY into PlyObject (took {elapsed_time:.2f} s)')\n",
        "  print(f'\\nVertice count: {len(obj.vertices)}')\n",
        "  print(f'Face count: {len(obj.faces)}')\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "ArkHJMUYLgjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab258bc-5577-4728-9f77-0b024e303270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.30 s)\n",
            "\n",
            "Vertice count: 24644\n",
            "Face count: 29692\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.50 s)\n",
            "\n",
            "Vertice count: 12219\n",
            "Face count: 24308\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.63 s)\n",
            "\n",
            "Vertice count: 86406\n",
            "Face count: 28802\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.26 s)\n",
            "\n",
            "Vertice count: 13902\n",
            "Face count: 24149\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Processed pre-converted PLY into PlyObject (took 0.50 s)\n",
            "\n",
            "Vertice count: 13044\n",
            "Face count: 14070\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Remove plane artifacts (dataset specific cleaning) {vertical-output: true}\n",
        "\n",
        "# has a giant rectangular plane originally used as a background, will need to be filtered out\n",
        "has_plane_artifact = ['Abbadon', 'Blind Perdition', 'Ex Machina', 'Komodo-4FR', 'Nova Mortis', 'Trespasser', 'Vestian Dynasty', 'Vouchsafe', 'Hereafter']\n",
        "\n",
        "# Remove plane artifact from manually specified objects\n",
        "\n",
        "# Could also iterate over all objects and use exceptions,\n",
        "# but would need to give delete_plane function stricter pre-deletion checking\n",
        "for obj in ply_objs:\n",
        "  if obj.name in has_plane_artifact:\n",
        "    has_plane_artifact.remove(obj.name)\n",
        "\n",
        "    obj_range = obj.dimension_range()\n",
        "\n",
        "    print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "    print(\"Dimension Range:\")\n",
        "    print('X:', obj_range['x'])\n",
        "    print('Y:', obj_range['y'])\n",
        "    print('Z:', obj_range['z'])\n",
        "\n",
        "    try:\n",
        "      start_time = time.time()\n",
        "\n",
        "      obj.delete_plane()\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "\n",
        "      obj_range = obj.dimension_range()\n",
        "\n",
        "      print(f\"\\nPlane detected and deleted (took {elapsed_time:.2f} s)\")\n",
        "      print(f\"\\nDimension Range (plane deleted in {elapsed_time:.2f} s):\")\n",
        "      print('X:', obj_range['x'])\n",
        "      print('Y:', obj_range['y'])\n",
        "      print('Z:', obj_range['z'])\n",
        "    except ValueError:\n",
        "      print(f\"\\nNo plane found!\")\n",
        "\n",
        "    print('-' * 50)\n",
        "\n",
        "if len(has_plane_artifact) > 0:\n",
        "  print(f\"\\nThe following manually-specified models were not hit (check for typos): {has_plane_artifact}\")"
      ],
      "metadata": {
        "id": "0_KXsecmwm8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74371895-713e-435d-a092-4008aa02264b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Blind Perdition\n",
            "\n",
            "Dimension Range:\n",
            "X: 1.831408\n",
            "Y: 0.47424900000000003\n",
            "Z: 3.43575\n",
            "\n",
            "Plane detected and deleted (took 0.06 s)\n",
            "\n",
            "Dimension Range (plane deleted in 0.06 s):\n",
            "X: 0.10221\n",
            "Y: 0.46234\n",
            "Z: 1.319475\n",
            "--------------------------------------------------\n",
            "\n",
            "The following manually-specified models were not hit (check for typos): ['Abbadon', 'Ex Machina', 'Komodo-4FR', 'Nova Mortis', 'Trespasser', 'Vestian Dynasty', 'Vouchsafe', 'Hereafter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Categorize face data to check for any bad n-gons\n",
        "\n",
        "face_lengths = {}\n",
        "\n",
        "for obj in ply_objs:\n",
        "  temp_face_lengths = obj.categorize_faces()\n",
        "\n",
        "  for length, count in temp_face_lengths.items():\n",
        "    if length in face_lengths:\n",
        "      face_lengths[length] += count\n",
        "    else:\n",
        "      face_lengths[length] = count\n",
        "\n",
        "for length, count in face_lengths.items():\n",
        "  print(f'Faces with {length} vertices: {count} instances')\n",
        "\n",
        "for length, count in face_lengths.items():\n",
        "  if length < 3 or length > 4:\n",
        "    raise Exception(f'\\nFace of unsupported size {length}!')"
      ],
      "metadata": {
        "id": "k-6q7ECD6Laa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3402c1ec-aea0-4df0-fde8-ad2ed1aff845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faces with 3 vertices: 119728 instances\n",
            "Faces with 4 vertices: 1292 instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert any objects with square faces to tris {vertical-output: true}\n",
        "\n",
        "# this could probably done using bpy before exporting as a PLY but this allows this to be done for any PlyObject\n",
        "\n",
        "face_lengths = {}\n",
        "square_obj_count = 0\n",
        "for obj in ply_objs:\n",
        "\n",
        "  face_data = obj.categorize_faces()\n",
        "\n",
        "  if 4 in face_data:\n",
        "    square_obj_count = square_obj_count + 1\n",
        "\n",
        "    print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "    if 3 in face_data:\n",
        "      print(f\"Tris: {face_data[3]}\")\n",
        "    else:\n",
        "      print(f\"Squares: 0\")\n",
        "    print(f\"Squares: {face_data[4]}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    obj.squares_to_tris()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f'\\nConverted square faces to tris (took {elapsed_time:.2f} s)\\n')\n",
        "    face_data = obj.categorize_faces()\n",
        "    print(f\"Tris: {face_data[3]}\")\n",
        "    if 4 in face_data:\n",
        "      print(f\"Squares: {face_data[4]}\")\n",
        "    else:\n",
        "      print(f\"Squares: 0\")\n",
        "\n",
        "    print('-' * 50)\n",
        "\n",
        "if square_obj_count == 0:\n",
        "  print(f\"No objects containing squares found. All objects contain only tris.\")"
      ],
      "metadata": {
        "id": "Px_DDxfNe-gk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a5226e-60b0-4913-9131-c44748eb5714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Prometheus Lens\n",
            "\n",
            "Tris: 22857\n",
            "Squares: 1292\n",
            "\n",
            "Converted square faces to tris (took 0.01 s)\n",
            "\n",
            "Tris: 25441\n",
            "Squares: 0\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Merge overlapping/duplicate vertices and faces {vertical-output: true}\n",
        "\n",
        "for obj in ply_objs:\n",
        "  initial_vertex_count = len(obj.vertices)\n",
        "  initial_face_count = len(obj.faces)\n",
        "\n",
        "  print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "  print(f'Vertex count: {initial_vertex_count}')\n",
        "  print(f'Face count: {initial_face_count}')\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  obj.remove_overlapping()\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  print(f'\\nDuplicate vertices: {initial_vertex_count - len(obj.vertices)}')\n",
        "  print(f'Duplicate faces: {initial_face_count - len(obj.faces)}')\n",
        "  print(f'\\nMerged any overlaping vertices and faces (took {elapsed_time:.2f} s)')\n",
        "  print(f'\\nVertex count: {len(obj.vertices)}')\n",
        "  print(f'Face count: {len(obj.faces)}')\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "xi9CjvimyRRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e03eb1-4ba9-479f-c206-1608464c18ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Vertex count: 24644\n",
            "Face count: 29692\n",
            "\n",
            "Duplicate vertices: 8148\n",
            "Duplicate faces: 3\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.09 s)\n",
            "\n",
            "Vertex count: 16496\n",
            "Face count: 29689\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Vertex count: 12219\n",
            "Face count: 24308\n",
            "\n",
            "Duplicate vertices: 0\n",
            "Duplicate faces: 0\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.05 s)\n",
            "\n",
            "Vertex count: 12219\n",
            "Face count: 24308\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Vertex count: 86406\n",
            "Face count: 28802\n",
            "\n",
            "Duplicate vertices: 78934\n",
            "Duplicate faces: 609\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.18 s)\n",
            "\n",
            "Vertex count: 7472\n",
            "Face count: 28193\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Vertex count: 13902\n",
            "Face count: 25441\n",
            "\n",
            "Duplicate vertices: 1312\n",
            "Duplicate faces: 44\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.05 s)\n",
            "\n",
            "Vertex count: 12590\n",
            "Face count: 25397\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Vertex count: 13040\n",
            "Face count: 14069\n",
            "\n",
            "Duplicate vertices: 5233\n",
            "Duplicate faces: 0\n",
            "\n",
            "Merged any overlaping vertices and faces (took 0.03 s)\n",
            "\n",
            "Vertex count: 7807\n",
            "Face count: 14069\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Center the objects to the origin {vertical-output: true}\n",
        "\n",
        "for obj in ply_objs:\n",
        "    start_time = time.time()\n",
        "\n",
        "    obj.center_object()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "\n",
        "    print(f'Centered to origin (took {elapsed_time:.2f} s)')\n",
        "\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "5kHJGPW1a3ks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9727642-c7ee-4fb2-e694-f7461e57d839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Centered to origin (took 0.03 s)\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Centered to origin (took 0.01 s)\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Centered to origin (took 0.02 s)\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Centered to origin (took 0.02 s)\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Centered to origin (took 0.01 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Normalize individual object scale to perfectly fit boundaries {vertical-output: true}\n",
        "\n",
        "for obj in ply_objs:\n",
        "\n",
        "  extrema = obj.get_value_extrema()\n",
        "\n",
        "  print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "  print(\"Extrema in any Dimension:\")\n",
        "  print(f\"Minimum: {extrema['min']}\")\n",
        "  print(f\"Maximum: {extrema['max']}\")\n",
        "\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  obj.normalize_scale()\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  print(f'\\nNormalised object scale to boundaries (took {elapsed_time:.2f} s)')\n",
        "\n",
        "  extrema = obj.get_value_extrema()\n",
        "\n",
        "  print(\"\\nExtrema in any Dimension:\")\n",
        "  print(f\"Minimum: {extrema['min']}\")\n",
        "  print(f\"Maximum: {extrema['max']}\")\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "kdzIJHgumSYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18008890-e407-49ee-bf94-9bad346b5292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "\n",
            "Normalised object scale to boundaries (took 0.02 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -0.1960095\n",
            "Maximum: 0.1960095\n",
            "\n",
            "Normalised object scale to boundaries (took 0.02 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -0.4440425\n",
            "Maximum: 0.4440425\n",
            "\n",
            "Normalised object scale to boundaries (took 0.01 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "\n",
            "Normalised object scale to boundaries (took 0.02 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -0.6597375\n",
            "Maximum: 0.6597375\n",
            "\n",
            "Normalised object scale to boundaries (took 0.01 s)\n",
            "\n",
            "Extrema in any Dimension:\n",
            "Minimum: -1.0\n",
            "Maximum: 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate minimum volume of any object\n",
        "\n",
        "volumes = {}\n",
        "min_volume = float('inf')\n",
        "volume_calc_times = {}\n",
        "\n",
        "for obj in ply_objs:\n",
        "    start_time = time.time()\n",
        "\n",
        "    volumes[obj.name] = obj.calculate_volume()\n",
        "\n",
        "    volume_calc_times[obj.name] = time.time() - start_time\n",
        "\n",
        "    min_volume = min(min_volume, volumes[obj.name])\n",
        "\n",
        "print(f'Global minimum volume: {min_volume:.6f}')"
      ],
      "metadata": {
        "id": "jnfWd69JS7rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50089940-4761-40c5-cbdc-06225391acf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global minimum volume: 0.028479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Scale each object to match the minimum global volume {vertical-output: true}\n",
        "\n",
        "# just scaled to boundaries so the objects must be scaled down, not up\n",
        "\n",
        "import math\n",
        "\n",
        "for obj in ply_objs:\n",
        "    print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "    print(f'Volume calculated as: {volumes[obj.name]:.6f} (took {volume_calc_times[obj.name]:.2f} s)')\n",
        "    start_time = time.time()\n",
        "\n",
        "    to_scale = math.pow(min_volume / volumes[obj.name], 1/3)\n",
        "    obj.scale(to_scale, to_scale, to_scale)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'\\nScaled by {to_scale} (took {elapsed_time:.2f} s)')\n",
        "    start_time = time.time()\n",
        "\n",
        "    volume = obj.calculate_volume()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'\\nVolume calculated as: {volume:.6f} (took {elapsed_time:.2f} s)')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "tvQGDyWRMYBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2da568-ec5a-470c-fbc1-1d468e73385f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.08 s)\n",
            "\n",
            "Scaled by 1.0 (took 0.01 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.07 s)\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Volume calculated as: 0.272199 (took 0.06 s)\n",
            "\n",
            "Scaled by 0.47121092908297674 (took 0.01 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.05 s)\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Volume calculated as: 0.065950 (took 0.07 s)\n",
            "\n",
            "Scaled by 0.7558567953805982 (took 0.01 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.07 s)\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Volume calculated as: 0.050188 (took 0.05 s)\n",
            "\n",
            "Scaled by 0.8278999959990196 (took 0.01 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.06 s)\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Volume calculated as: 0.044829 (took 0.03 s)\n",
            "\n",
            "Scaled by 0.8596580597884671 (took 0.00 s)\n",
            "\n",
            "Volume calculated as: 0.028479 (took 0.03 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Analyze dimension ranges to pseudo-verify object orientation\n",
        "\n",
        "# could do anywhere before doing global stretch\n",
        "# choosing to do it after all other processing\n",
        "\n",
        "# used for checking if I missed any rotation overrides\n",
        "count_smallest = {'x': 0, 'y': 0, 'z': 0}\n",
        "count_middle = {'x': 0, 'y': 0, 'z': 0}\n",
        "count_largest = {'x': 0, 'y': 0, 'z': 0}\n",
        "\n",
        "for obj in ply_objs:\n",
        "\n",
        "  obj_range = obj.dimension_range()\n",
        "\n",
        "  # count the podium placings of ranges for every dimension\n",
        "  sorted_keys = sorted(obj_range, key=obj_range.get)\n",
        "  smallest_key = sorted_keys[0]\n",
        "  middle_key = sorted_keys[1]\n",
        "  largest_key = sorted_keys[2]\n",
        "\n",
        "  count_smallest[smallest_key] += 1\n",
        "  count_middle[middle_key] += 1\n",
        "  count_largest[largest_key] += 1\n",
        "\n",
        "print(\"Times with largest dimensional range:\")\n",
        "print(f\"X: {count_largest['x']}\")\n",
        "print(f\"Y: {count_largest['y']}\")\n",
        "print(f\"Z: {count_largest['z']}\")\n",
        "\n",
        "print(\"\\nTimes with middle dimensional range:\")\n",
        "print(f\"X: {count_middle['x']}\")\n",
        "print(f\"Y: {count_middle['y']}\")\n",
        "print(f\"Z: {count_middle['z']}\")\n",
        "\n",
        "print(\"\\nTimes with smallest dimensional range:\")\n",
        "print(f\"X: {count_smallest['x']}\")\n",
        "print(f\"Y: {count_smallest['y']}\")\n",
        "print(f\"Z: {count_smallest['z']}\")"
      ],
      "metadata": {
        "id": "G_IkGyZ3Og6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e5af7f-7b2e-4deb-bf38-95c77188f5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Times with largest dimensional range:\n",
            "X: 0\n",
            "Y: 0\n",
            "Z: 5\n",
            "\n",
            "Times with middle dimensional range:\n",
            "X: 0\n",
            "Y: 5\n",
            "Z: 0\n",
            "\n",
            "Times with smallest dimensional range:\n",
            "X: 5\n",
            "Y: 0\n",
            "Z: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate global stretch values given global dimensional extrema\n",
        "\n",
        "min_x = min_y = min_z = float('inf')\n",
        "max_x = max_y = max_z = float('-inf')\n",
        "\n",
        "for obj in ply_objs:\n",
        "  for vertex in obj.vertices:\n",
        "    min_x = min(min_x, vertex.x)\n",
        "    min_y = min(min_y, vertex.y)\n",
        "    min_z = min(min_z, vertex.z)\n",
        "    max_x = max(max_x, vertex.x)\n",
        "    max_y = max(max_y, vertex.y)\n",
        "    max_z = max(max_z, vertex.z)\n",
        "\n",
        "scale_x, scale_y, scale_z = 2/(max_x - min_x), 2/(max_y - min_y), 2/(max_z - min_z)\n",
        "\n",
        "print(f\"Dimensional minima:\")\n",
        "print(f\"X: {min_x}\")\n",
        "print(f\"Y: {min_y}\")\n",
        "print(f\"Z: {min_z}\")\n",
        "\n",
        "print(f\"\\nDimensional maxima:\")\n",
        "print(f\"X: {max_x}\")\n",
        "print(f\"Y: {max_y}\")\n",
        "print(f\"Z: {max_z}\\n\")\n",
        "\n",
        "print('-' * 50)\n",
        "\n",
        "print(f\"\\nDerived Stretch Value:\")\n",
        "print(f\"X: {scale_x}\")\n",
        "print(f\"Y: {scale_y}\")\n",
        "print(f\"Z: {scale_z}\")"
      ],
      "metadata": {
        "id": "zSFUwzZuOFvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38607d30-144f-4577-9a23-3251bda117b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensional minima:\n",
            "X: -0.12194939584895402\n",
            "Y: -0.33554149999999994\n",
            "Z: -1.0\n",
            "\n",
            "Dimensional maxima:\n",
            "X: 0.12194939584895394\n",
            "Y: 0.33554149999999994\n",
            "Z: 1.0\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Derived Stretch Value:\n",
            "X: 8.200122624950072\n",
            "Y: 2.9802572856114673\n",
            "Z: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Stretch non-bounded dimensions using constant values to tighten scope {vertical-output: true}\n",
        "\n",
        "global_stretch_scale_x = scale_x\n",
        "global_stretch_scale_y = scale_y\n",
        "global_stretch_scale_z = scale_z\n",
        "\n",
        "for obj in ply_objs:\n",
        "  obj_range = obj.dimension_range()\n",
        "\n",
        "  print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "  print(\"Dimension Range:\")\n",
        "  print('X:', obj_range['x'])\n",
        "  print('Y:', obj_range['y'])\n",
        "  print('Z:', obj_range['z'])\n",
        "\n",
        "  start_time = time.time()\n",
        "  obj.stretch_to_max()\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  obj_range = obj.dimension_range()\n",
        "\n",
        "  print(f\"\\nStretched non-bounded dimensions using global constants (took {elapsed_time:.2f} s):\")\n",
        "  print(f\"\\nDimension Range (stretched in {elapsed_time:.2f} s):\")\n",
        "  print('X:', obj_range['x'])\n",
        "  print('Y:', obj_range['y'])\n",
        "  print('Z:', obj_range['z'])\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "YBSTpRUREVt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c6f6a3a-a0eb-46da-da44-96bc42829b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.14593999999999996\n",
            "Y: 0.6710829999999999\n",
            "Z: 2.0\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.01 s):\n",
            "\n",
            "Dimension Range (stretched in 0.01 s):\n",
            "X: 1.1967258958852132\n",
            "Y: 2.0\n",
            "Z: 2.0\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.14158240522736318\n",
            "Y: 0.5979953838336013\n",
            "Z: 0.9424218581659535\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.01 s):\n",
            "\n",
            "Dimension Range (stretched in 0.01 s):\n",
            "X: 1.1609930843997502\n",
            "Y: 1.7821800994321162\n",
            "Z: 0.9424218581659535\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.24389879169790796\n",
            "Y: 0.5689507899464725\n",
            "Z: 1.5117135907611965\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.01 s):\n",
            "\n",
            "Dimension Range (stretched in 0.01 s):\n",
            "X: 2.0\n",
            "Y: 1.6956197368923742\n",
            "Z: 1.5117135907611965\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.12211874839298595\n",
            "Y: 0.5537732888229542\n",
            "Z: 1.6557999919980393\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.01 s):\n",
            "\n",
            "Dimension Range (stretched in 0.01 s):\n",
            "X: 1.0013887116279094\n",
            "Y: 1.6503868785916325\n",
            "Z: 1.6557999919980393\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Dimension Range:\n",
            "X: 0.13318274357752768\n",
            "Y: 0.602443104056689\n",
            "Z: 1.7193161195769342\n",
            "\n",
            "Stretched non-bounded dimensions using global constants (took 0.00 s):\n",
            "\n",
            "Dimension Range (stretched in 0.00 s):\n",
            "X: 1.0921148288630087\n",
            "Y: 1.7954354500313348\n",
            "Z: 1.7193161195769342\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Offset objects to pin Z and Y object maximums to upper boundaries to localize similar features (dataset specific scope reduction) {vertical-output: true}\n",
        "\n",
        "# pinning all objects against back wall\n",
        "\n",
        "for obj in ply_objs:\n",
        "  start_time = time.time()\n",
        "\n",
        "  max_values = obj.get_max_values()\n",
        "\n",
        "  max_z = max_values['z']\n",
        "  z_offset = 1 - max_z\n",
        "\n",
        "  max_y = max_values['y']\n",
        "  y_offset = 1 - max_y\n",
        "\n",
        "  obj.translate(0, y_offset, z_offset)\n",
        "\n",
        "  max_values = obj.get_max_values()\n",
        "  new_max_z = max_values['z']\n",
        "  new_max_y = max_values['y']\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "  print(f\"\\nMaximum Z: {max_z}\")\n",
        "  print(f\"\\nMaximum Y: {max_y}\")\n",
        "  print(f'\\nOffset object to boundaries (took {elapsed_time:.2f} s)')\n",
        "  print(f\"\\nMaximum Z: {new_max_z}\")\n",
        "  print(f\"\\nMaximum Y: {new_max_y}\")\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "-ZOVWrpTsFpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78fe6415-7dcd-403f-91ac-ad92ac3e6c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "\n",
            "Offset object to boundaries (took 0.03 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Maximum Z: 0.47121092908297674\n",
            "\n",
            "Maximum Y: 0.8910900497160581\n",
            "\n",
            "Offset object to boundaries (took 0.02 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Maximum Z: 0.7558567953805982\n",
            "\n",
            "Maximum Y: 0.8478098684461871\n",
            "\n",
            "Offset object to boundaries (took 0.02 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Maximum Z: 0.8278999959990196\n",
            "\n",
            "Maximum Y: 0.8251934392958161\n",
            "\n",
            "Offset object to boundaries (took 0.02 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Maximum Z: 0.8596580597884671\n",
            "\n",
            "Maximum Y: 0.8977177250156672\n",
            "\n",
            "Offset object to boundaries (took 0.02 s)\n",
            "\n",
            "Maximum Z: 1.0\n",
            "\n",
            "Maximum Y: 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Verify that dataset dimensional extrema are properly constrained\n",
        "\n",
        "track_min = {'x': float('inf'), 'y': float('inf'), 'z': float('inf')}\n",
        "track_max = {'x': float('-inf'), 'y': float('-inf'), 'z': float('-inf')}\n",
        "\n",
        "for obj in ply_objs:\n",
        "  max_values = obj.get_max_values()\n",
        "  min_values = obj.get_min_values()\n",
        "\n",
        "  for dimension in ['x', 'y', 'z']:\n",
        "    track_min[dimension] = min(track_min[dimension], min_values[dimension])\n",
        "    track_max[dimension] = max(track_max[dimension], max_values[dimension])\n",
        "\n",
        "print(\"Dimensional Minima:\")\n",
        "print('X:', track_min['x'])\n",
        "print('Y:', track_min['y'])\n",
        "print('Z:', track_min['z'])\n",
        "\n",
        "print(\"\\nDimensional Maxima:\")\n",
        "print('X:', track_max['x'])\n",
        "print('Y:', track_max['y'])\n",
        "print('Z:', track_max['z'])"
      ],
      "metadata": {
        "id": "mZkkQKFtbXBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8984f04e-81e7-49d2-da84-41e0ff159088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensional Minima:\n",
            "X: -1.0000000000000002\n",
            "Y: -1.0\n",
            "Z: -1.0\n",
            "\n",
            "Dimensional Maxima:\n",
            "X: 0.9999999999999997\n",
            "Y: 1.0\n",
            "Z: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sort order of vertices and faces numerically to remove arbitrary sample noise {vertical-output: true}\n",
        "\n",
        "for obj in ply_objs:\n",
        "    start_time = time.time()\n",
        "\n",
        "    obj.sort_tri_data(True)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "\n",
        "    print(f'Sorted object vertices and faces numerically (took {elapsed_time:.2f} s)')\n",
        "\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "cLgEiIWuPW1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10d1c1d-9c60-4f42-8bc0-4b46b31501b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Sorted object vertices and faces numerically (took 0.89 s)\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Sorted object vertices and faces numerically (took 0.37 s)\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Sorted object vertices and faces numerically (took 1.00 s)\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Sorted object vertices and faces numerically (took 0.39 s)\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Sorted object vertices and faces numerically (took 0.54 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export normalized PLY files {vertical-output: true}\n",
        "\n",
        "!rm -r {NORMALIZED_PLY} 2>/dev/null\n",
        "!mkdir {NORMALIZED_PLY}\n",
        "\n",
        "for obj in ply_objs:\n",
        "  start_time = time.time()\n",
        "\n",
        "  # todo: remove before final model use\n",
        "  obj_copy = copy.copy(obj)\n",
        "  obj_copy.revert_stretch()\n",
        "\n",
        "  obj_copy.save_file(NORMALIZED_PLY + obj.name + '.ply')\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "\n",
        "  print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "  print(f'Exported normalized PLY file (took {elapsed_time:.2f} s)')\n",
        "  print('-' * 50)\n",
        "\n",
        "# note - resets normalized_ply directory in drive if set to True\n",
        "send_to_google_drive = False\n",
        "if send_to_google_drive:\n",
        "  !rm -f {DRIVE + 'normalized_ply/*'} 2>/dev/null\n",
        "  !cp -r {NORMALIZED_PLY} {DRIVE}"
      ],
      "metadata": {
        "id": "0RtTCSIT5OmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76243521-bc12-4b0d-8d44-7e9c88060c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Line in the Sand\n",
            "\n",
            "Exported normalized PLY file (took 0.24 s)\n",
            "--------------------------------------------------\n",
            "Object: Rat King\n",
            "\n",
            "Exported normalized PLY file (took 0.15 s)\n",
            "--------------------------------------------------\n",
            "Object: Outbreak Perfected\n",
            "\n",
            "Exported normalized PLY file (took 0.14 s)\n",
            "--------------------------------------------------\n",
            "Object: Prometheus Lens\n",
            "\n",
            "Exported normalized PLY file (took 0.16 s)\n",
            "--------------------------------------------------\n",
            "Object: Blind Perdition\n",
            "\n",
            "Exported normalized PLY file (took 0.08 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset size padding as random variations"
      ],
      "metadata": {
        "id": "h19Agf8YtbE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define imports and functions\n",
        "\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "use_partial_dataset = True\n",
        "variations_use_gdrive = False\n",
        "\n",
        "def extract_num_from_string(filenames, string):\n",
        "  numbers = set()\n",
        "  for filename in filenames:\n",
        "    if string in filename and filename.endswith(\".npy\"):\n",
        "      number = ''.join(filter(str.isdigit, filename))\n",
        "      if number.isdigit():\n",
        "        numbers.add(int(number))\n",
        "  return numbers\n",
        "\n",
        "\n",
        "def highest_variation_index_in_path(folder_path):\n",
        "  filenames = os.listdir(folder_path)\n",
        "  vertex_numbers = extract_num_from_string(filenames, 'vertex_input_list_')\n",
        "  face_numbers = extract_num_from_string(filenames, 'face_input_list_')\n",
        "  completed_variations = sorted(list(vertex_numbers & face_numbers))\n",
        "\n",
        "  if not completed_variations:\n",
        "    return -1\n",
        "\n",
        "  last_consecutive_index = -1\n",
        "  for i in range(0, len(completed_variations)):\n",
        "    if completed_variations[i] == i:\n",
        "      last_consecutive_index = i\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  return last_consecutive_index"
      ],
      "metadata": {
        "id": "9jTj6kxEpIRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import pre-normalized PLY files {vertical-output: true}\n",
        "\n",
        "# pulling from google drive\n",
        "\n",
        "ply_objs=[]\n",
        "\n",
        "idx = 0\n",
        "for f in os.listdir(DRIVE + 'normalized_ply/'):\n",
        "  if f.endswith('.ply') and ((not use_partial_dataset) or f.startswith('Lu')):\n",
        "    start_time = time.time()\n",
        "    obj = PlyObject.from_file(os.path.join(DRIVE + 'normalized_ply/', f), idx)\n",
        "    ply_objs.append(obj)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    idx += 1\n",
        "    print(f\"Object: {obj.name}\\n\")\n",
        "    print(f'Processed pre-normalized PLY file into PlyObject (took {elapsed_time:.2f} s)')\n",
        "    print(f'\\nVertice count: {len(obj.vertices)}')\n",
        "    print(f'Face count: {len(obj.faces)}')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "mP9vVnu5JaRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c28e270-41ea-40c9-c0f3-b960c10584fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: Luna's Howl\n",
            "\n",
            "Processed pre-normalized PLY file into PlyObject (took 0.04 s)\n",
            "\n",
            "Vertice count: 5502\n",
            "Face count: 10168\n",
            "--------------------------------------------------\n",
            "Object: Lumina\n",
            "\n",
            "Processed pre-normalized PLY file into PlyObject (took 0.47 s)\n",
            "\n",
            "Vertice count: 9249\n",
            "Face count: 17312\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Analyze object tri data to determine global target padding size and autoencoder input layer size\n",
        "\n",
        "max_vertices = 0\n",
        "max_faces = 0\n",
        "\n",
        "for obj in ply_objs:\n",
        "  max_vertices = max(max_vertices, len(obj.vertices))\n",
        "  max_faces = max(max_faces, len(obj.faces))\n",
        "\n",
        "print(f'Object count: {len(ply_objs)}')\n",
        "\n",
        "\n",
        "print(f'\\nMaximum vertices for any object: {max_vertices}')\n",
        "print(f'Maximum faces for any object: {max_faces}')\n",
        "\n",
        "vertex_target_count = int(max_vertices * 1.2)\n",
        "face_target_count = int(max_faces * 1.2)\n",
        "\n",
        "# make max_faces even so we can encode 2 faces onto eachother\n",
        "#face_target_count = face_target_count + face_target_count % 2\n",
        "\n",
        "print(f'\\n120% of vertex count: {vertex_target_count}')\n",
        "print(f'120% of face count: {face_target_count}')\n",
        "\n",
        "vertex_input_size = vertex_target_count\n",
        "#face_input_size = face_target_count // 2\n",
        "face_input_size = face_target_count\n",
        "\n",
        "\n",
        "print(f'\\nVertex input layer width: {vertex_input_size}')\n",
        "print(f'Face input layer width (2 faces per): {face_input_size}')\n",
        "\n",
        "print(f'\\nVertex neurons (3 per): {vertex_input_size * 3}')\n",
        "print(f'Face neurons (3 per): {face_input_size * 3}')\n",
        "print(f'Total Input Neurons: {vertex_input_size * 3 + face_input_size * 3}\\n')"
      ],
      "metadata": {
        "id": "os84buC2qlnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ff9f45-e9a1-494a-edb7-ad9e693236c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object count: 2\n",
            "\n",
            "Maximum vertices for any object: 9249\n",
            "Maximum faces for any object: 17312\n",
            "\n",
            "120% of vertex count: 11098\n",
            "120% of face count: 20774\n",
            "\n",
            "Vertex input layer width: 11098\n",
            "Face input layer width (2 faces per): 20774\n",
            "\n",
            "Vertex neurons (3 per): 33294\n",
            "Face neurons (3 per): 62322\n",
            "Total Input Neurons: 95616\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Randomly pad PlyObjects to match global target size and export to numpy array files {vertical-output: true}\n",
        "\n",
        "# batch processing notes (post-fix):\n",
        "# all 129 items\n",
        "# 100 variations\n",
        "# 12047 seconds\n",
        "# ~1 second per variation\n",
        "\n",
        "# if this is false, it will iterate the file name\n",
        "delete_old_file_variations = False\n",
        "# todo: final model should be very large\n",
        "variations_to_generate = 2\n",
        "\n",
        "\n",
        "if variations_use_gdrive:\n",
        "  if use_partial_dataset:\n",
        "    path_to_save = DRIVE + \"variation_gen_partial/\"\n",
        "  else:\n",
        "    path_to_save = DRIVE + \"variation_gen/\"\n",
        "else:\n",
        "  path_to_save = DIR + \"variation_gen/\"\n",
        "\n",
        "if delete_old_file_variations:\n",
        "  !rm -r {path_to_save} 2>/dev/null\n",
        "  !mkdir {path_to_save} 2>/dev/null\n",
        "  idx = 0\n",
        "else:\n",
        "  !mkdir {path_to_save} 2>/dev/null\n",
        "  idx = highest_variation_index_in_path(path_to_save) + 1\n",
        "\n",
        "for obj in ply_objs:\n",
        "  initial_vertex_count = len(obj.vertices)\n",
        "  initial_face_count = len(obj.faces)\n",
        "  print(f\"Object ({obj.id + 1}/{len(ply_objs)}): {obj.name}\\n\")\n",
        "  print(f'Vertex count: {len(obj.vertices)}')\n",
        "  print(f'Face count: {len(obj.faces)}')\n",
        "  start_time = time.time()\n",
        "\n",
        "  variations = obj.export_random_to_model(variations_to_generate, vertex_target_count, face_target_count)\n",
        "\n",
        "  variation_vertices = variations['vertices']\n",
        "  variation_faces = variations['faces']\n",
        "\n",
        "  first_idx = idx\n",
        "\n",
        "  for i in range(len(variation_vertices)):\n",
        "    np.save(f\"{path_to_save}vertex_input_list_{idx}.npy\", variation_vertices[i])\n",
        "    np.save(f\"{path_to_save}face_input_list_{idx}.npy\", variation_faces[i])\n",
        "\n",
        "    idx += 1\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "  if variations_to_generate == 1:\n",
        "    print(f'\\nFor 1 variation (file index {first_idx})')\n",
        "  else:\n",
        "    print(f'\\nFor {variations_to_generate} variations (file indexes {first_idx}-{idx})')\n",
        "\n",
        "  print(f'Added vertices: {vertex_target_count - initial_vertex_count}')\n",
        "  print(f'Added faces: {face_target_count - initial_face_count}')\n",
        "  if variations_to_generate == 1:\n",
        "      print(f'\\nGenerated 1 random variation (took {elapsed_time:.2f} s)')\n",
        "  else:\n",
        "    print(f'\\nGenerated {variations_to_generate} random variation(s) (took {elapsed_time:.2f} s)')\n",
        "  print('-' * 50)\n",
        "\n",
        "\n",
        "# Run this step before defining VAE models to avoid system memory cap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO6fkfz5gLkA",
        "outputId": "b54e09ae-94f8-4380-c2bc-a4d57ec88526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object (1/2): Luna's Howl\n",
            "\n",
            "Vertex count: 11098\n",
            "Face count: 19587\n",
            "\n",
            "For 2 variations (file indexes 0-2)\n",
            "Added vertices: 0\n",
            "Added faces: 1187\n",
            "\n",
            "Generated 2 random variation(s) (took 1.42 s)\n",
            "--------------------------------------------------\n",
            "Object (2/2): Lumina\n",
            "\n",
            "Vertex count: 9249\n",
            "Face count: 17312\n",
            "\n",
            "For 2 variations (file indexes 2-4)\n",
            "Added vertices: 1849\n",
            "Added faces: 3462\n",
            "\n",
            "Generated 2 random variation(s) (took 1.92 s)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import pre-computed random variations (to allow for runtime restart) {vertical-output: true}\n",
        "\n",
        "if variations_use_gdrive:\n",
        "  if use_partial_dataset:\n",
        "    path_to_save = DRIVE + \"variation_gen_partial/\"\n",
        "  else:\n",
        "    path_to_save = DRIVE + \"variation_gen/\"\n",
        "else:\n",
        "  path_to_save = DIR + \"variation_gen/\"\n",
        "\n",
        "dataset_size = highest_variation_index_in_path(path_to_save) + 1\n",
        "\n",
        "if dataset_size > 0:\n",
        "  mmapped_file = np.lib.format.open_memmap(f\"{path_to_save}vertex_input_list_0.npy\")\n",
        "  vertex_input_size = mmapped_file.shape[0]\n",
        "\n",
        "  mmapped_file = np.lib.format.open_memmap(f\"{path_to_save}face_input_list_0.npy\")\n",
        "  face_input_size = mmapped_file.shape[0]\n",
        "\n",
        "vertex_input_list = np.zeros((dataset_size, vertex_input_size, 3))\n",
        "face_input_list = np.zeros((dataset_size, face_input_size, 3))\n",
        "\n",
        "for idx in range(dataset_size):\n",
        "  vertex_input_list[idx] = np.load(f\"{path_to_save}vertex_input_list_{idx}.npy\")\n",
        "  face_input_list[idx] = np.load(f\"{path_to_save}face_input_list_{idx}.npy\")\n",
        "\n",
        "  print(f'Vertex data file: vertex_input_list_{idx}.npy')\n",
        "  print(f'Face data file: face_input_list_{idx}.npy')\n",
        "  print(f'\\nImported variation {idx + 1}/{dataset_size}')\n",
        "  print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9Y8y9017ZX7",
        "outputId": "a2917eeb-eb6e-4fff-d6ee-47a925addd36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex data file: vertex_input_list_0.npy\n",
            "Face data file: face_input_list_0.npy\n",
            "\n",
            "Imported variation 1/4\n",
            "--------------------------------------------------\n",
            "Vertex data file: vertex_input_list_1.npy\n",
            "Face data file: face_input_list_1.npy\n",
            "\n",
            "Imported variation 2/4\n",
            "--------------------------------------------------\n",
            "Vertex data file: vertex_input_list_2.npy\n",
            "Face data file: face_input_list_2.npy\n",
            "\n",
            "Imported variation 3/4\n",
            "--------------------------------------------------\n",
            "Vertex data file: vertex_input_list_3.npy\n",
            "Face data file: face_input_list_3.npy\n",
            "\n",
            "Imported variation 4/4\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Analyze random padding results to verify dataset validity\n",
        "\n",
        "print(f'Vertex list shape: {vertex_input_list.shape}')\n",
        "print(f'Face list shape: {face_input_list.shape}')\n",
        "print(f'Number of zeros in vertex_input_list: {np.count_nonzero(vertex_input_list==0)}')\n",
        "print(f'Number of zeros in face_input_list: {np.count_nonzero(face_input_list==0)}')\n",
        "\n",
        "print(f'\\nMean of vertex_input_list values: {np.mean(vertex_input_list)}, Std Dev: {np.std(vertex_input_list)}')\n",
        "print(f'Mean absolute value of vertex_input_list: {np.mean(np.abs(vertex_input_list))}')\n",
        "print(f'Mean of face_input_list values: {np.mean(face_input_list)}, Std Dev: {np.std(face_input_list)}')\n",
        "\n",
        "print(f'\\nNumber of NaNs in vertex_input_list: {np.isnan(vertex_input_list).sum()}')\n",
        "print(f'Number of NaNs in face_input_list: {np.isnan(face_input_list).sum()}')"
      ],
      "metadata": {
        "id": "gpzYauT5lCPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef90a1e0-9f1d-4788-88d6-8ad81449d1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex list shape: (4, 11098, 3)\n",
            "Face list shape: (4, 20774, 3)\n",
            "Number of zeros in vertex_input_list: 0\n",
            "Number of zeros in face_input_list: 26\n",
            "\n",
            "Mean of vertex_input_list values: 0.367421866525923, Std Dev: 0.3982629537978347\n",
            "Mean absolute value of vertex_input_list: 0.4498698553557993\n",
            "Mean of face_input_list values: 5541.2629849812265, Std Dev: 3223.9122948922786\n",
            "\n",
            "Number of NaNs in vertex_input_list: 0\n",
            "Number of NaNs in face_input_list: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder initialization and training"
      ],
      "metadata": {
        "id": "pv5jlq94ermA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define imports and sampling function\n",
        "\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.utils import pad_sequences\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.models import load_model\n",
        "import time\n",
        "try:\n",
        "  import wandb\n",
        "except ImportError:\n",
        "  !pip install wandb\n",
        "  import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "model_use_gdrive = False\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.random.normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "eDyw74K6umGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Normalize our final variation data\n",
        "\n",
        "vertex_input_list = tf.cast(vertex_input_list, tf.float32)\n",
        "face_input_list = tf.cast(face_input_list, tf.float32)\n",
        "\n",
        "normalizer_ver = preprocessing.Normalization(axis=-1)\n",
        "normalizer_face = preprocessing.Normalization(axis=-1)\n",
        "\n",
        "normalizer_ver.adapt(vertex_input_list)\n",
        "normalizer_face.adapt(face_input_list)\n",
        "\n",
        "mean_ver, variance_ver = normalizer_ver.mean, normalizer_ver.variance\n",
        "mean_face, variance_face = normalizer_face.mean, normalizer_face.variance\n",
        "\n",
        "vertex_input_list = normalizer_ver(vertex_input_list)\n",
        "face_input_list = normalizer_face(face_input_list)\n",
        "\n",
        "print(f'\\nMean of vertex_input_list values: {np.mean(vertex_input_list)}, Std Dev: {np.std(vertex_input_list)}')\n",
        "print(f'Mean of face_input_list values: {np.mean(face_input_list)}, Std Dev: {np.std(face_input_list)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btmbwc7nx1qp",
        "outputId": "4571f6f2-7dca-4dc4-cd60-23c1a0ed39ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean of vertex_input_list values: -4.078909796589869e-07, Std Dev: 1.0000001192092896\n",
            "Mean of face_input_list values: 1.4184929568727966e-05, Std Dev: 1.0000032186508179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize our run in wandb\n",
        "\n",
        "dataset_string = \"partial\" if use_partial_dataset else \"full\"\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"AutoCalibr\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      \"vertex_input_size\": vertex_input_size,\n",
        "      \"face_input_size\": face_input_size,\n",
        "      \"learning_rate\": 0.005,\n",
        "      \"dataset\": f\"{dataset_string}\",\n",
        "      \"global_size_multiplier\": 0.2,\n",
        "      \"vertex_size_multiplier\": 1.0,\n",
        "      \"face_size_multiplier\": 0.2,\n",
        "      \"layer_1_size_red\": 0.5,\n",
        "      \"layer_2_size_red\": 0.7,\n",
        "      \"combination_layer_1_size_red\": 0.7,\n",
        "      \"combination_layer_2_size_red\": 0.7,\n",
        "      \"latent_dim\": 32,\n",
        "      \"prenormalized_vertex_mean\": mean_ver,\n",
        "      \"prenormalized_vertex_variance\": variance_ver,\n",
        "      \"prenormalized_face_mean\": mean_face,\n",
        "      \"prenormalized_face_variance\": variance_face,\n",
        "    }\n",
        ")\n",
        "\n",
        "config = wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "PbWmHrgsoYzG",
        "outputId": "89efa2c6-acb8-49e0-a3ec-5a6d03048b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-25439d6cdae9>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# track hyperparameters and run metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     config={\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;34m\"vertex_input_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvertex_input_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;34m\"face_input_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mface_input_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vertex_input_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize encoder model\n",
        "\n",
        "ply_objs = None\n",
        "\n",
        "# should likely be max 0.2. Start around 0.05\n",
        "global_size_multiplier = config.global_size_multiplier\n",
        "\n",
        "vertex_size_multiplier = config.vertex_size_multiplier\n",
        "face_size_multiplier = config.face_size_multiplier\n",
        "\n",
        "layer_1_size_red = config.layer_1_size_red\n",
        "layer_2_size_red = config.layer_2_size_red\n",
        "combination_layer_1_size_red = config.combination_layer_1_size_red\n",
        "combination_layer_2_size_red = config.combination_layer_2_size_red\n",
        "\n",
        "latent_dim = config.latent_dim\n",
        "\n",
        "vertex_inputs = keras.Input(shape=(vertex_input_size, 3), name='vertex_input')\n",
        "face_inputs = keras.Input(shape=(face_input_size, 3), name='face_input')\n",
        "\n",
        "vertex_size_weighted = vertex_input_size * vertex_size_multiplier\n",
        "face_size_weighted = face_input_size * face_size_multiplier\n",
        "\n",
        "current_layer_multiplier = 3 * global_size_multiplier * layer_1_size_red\n",
        "vertex_layer = layers.Flatten(name='vertex_flatten')(vertex_inputs)\n",
        "face_layer = layers.Flatten(name='face_flatten')(face_inputs)\n",
        "\n",
        "vertex_layer = layers.Dense(vertex_size_weighted * current_layer_multiplier, activation=\"relu\", name='vertex_dense_1')(vertex_layer)\n",
        "face_layer = layers.Dense(face_size_weighted * current_layer_multiplier, activation=\"relu\", name='face_dense_1')(face_layer)\n",
        "\n",
        "current_layer_multiplier *= layer_2_size_red\n",
        "vertex_layer = layers.Dense(vertex_size_weighted * current_layer_multiplier, activation=\"relu\", name='vertex_dense_2')(vertex_layer)\n",
        "face_layer = layers.Dense(face_size_weighted * current_layer_multiplier, activation=\"relu\", name='face_dense_2')(face_layer)\n",
        "\n",
        "combined_layer = layers.Concatenate(name='concatenate')([vertex_layer, face_layer])\n",
        "concatenate_result_size = (vertex_size_weighted + face_size_weighted) * current_layer_multiplier\n",
        "vertex_layer = None\n",
        "face_layer = None\n",
        "\n",
        "current_layer_multiplier = combination_layer_1_size_red\n",
        "combined_layer = layers.Dense(concatenate_result_size * current_layer_multiplier, activation=\"relu\", name='combined_dense_1')(combined_layer)\n",
        "\n",
        "current_layer_multiplier *= combination_layer_2_size_red\n",
        "combined_layer = layers.Dense(concatenate_result_size * current_layer_multiplier, activation=\"relu\", name='combined_dense_2')(combined_layer)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(combined_layer)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(combined_layer)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "encoder = keras.Model((vertex_inputs, face_inputs), (z_mean, z_log_var, z), name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "x1I-hJ5cwatz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22c5cd2-9cf1-41bb-e722-e3845918f06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " vertex_input (InputLayer)      [(None, 11098, 3)]   0           []                               \n",
            "                                                                                                  \n",
            " face_input (InputLayer)        [(None, 20774, 3)]   0           []                               \n",
            "                                                                                                  \n",
            " vertex_flatten (Flatten)       (None, 33294)        0           ['vertex_input[0][0]']           \n",
            "                                                                                                  \n",
            " face_flatten (Flatten)         (None, 62322)        0           ['face_input[0][0]']             \n",
            "                                                                                                  \n",
            " vertex_dense_1 (Dense)         (None, 3329)         110839055   ['vertex_flatten[0][0]']         \n",
            "                                                                                                  \n",
            " face_dense_1 (Dense)           (None, 1246)         77654458    ['face_flatten[0][0]']           \n",
            "                                                                                                  \n",
            " vertex_dense_2 (Dense)         (None, 2330)         7758900     ['vertex_dense_1[0][0]']         \n",
            "                                                                                                  \n",
            " face_dense_2 (Dense)           (None, 872)          1087384     ['face_dense_1[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3202)         0           ['vertex_dense_2[0][0]',         \n",
            "                                                                  'face_dense_2[0][0]']           \n",
            "                                                                                                  \n",
            " combined_dense_1 (Dense)       (None, 2242)         7181126     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " combined_dense_2 (Dense)       (None, 1569)         3519267     ['combined_dense_1[0][0]']       \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 32)           50240       ['combined_dense_2[0][0]']       \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 32)           50240       ['combined_dense_2[0][0]']       \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 32)           0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 208,140,670\n",
            "Trainable params: 208,140,670\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize decoder model\n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim,), name='latent')\n",
        "\n",
        "decode_dense_multiplier = 3 * global_size_multiplier * layer_1_size_red * layer_2_size_red\n",
        "concatenate_result_size = (vertex_size_weighted + face_size_weighted) * decode_dense_multiplier\n",
        "current_layer_multiplier = combination_layer_1_size_red * combination_layer_2_size_red\n",
        "\n",
        "combined_layer = layers.Dense(concatenate_result_size * current_layer_multiplier, activation=\"relu\", name='from_latent_dense_1')(latent_inputs)\n",
        "\n",
        "current_layer_multiplier = combination_layer_1_size_red\n",
        "combined_layer = layers.Dense(concatenate_result_size * current_layer_multiplier, activation=\"relu\", name='from_latent_dense_2')(combined_layer)\n",
        "\n",
        "combined_layer = layers.Dense(concatenate_result_size, activation=\"relu\", name='from_latent_dense_3')(combined_layer)\n",
        "\n",
        "vertex_layer = layers.Dense(vertex_size_weighted * decode_dense_multiplier, activation=\"relu\", name='decode_vertex_dense_1')(combined_layer)\n",
        "face_layer = layers.Dense(face_size_weighted * decode_dense_multiplier, activation=\"relu\", name='decode_face_dense_1')(combined_layer)\n",
        "combined_layer = None\n",
        "\n",
        "decode_dense_multiplier /= layer_2_size_red\n",
        "vertex_layer = layers.Dense(vertex_size_weighted * decode_dense_multiplier, activation=\"relu\", name='decode_vertex_dense_2')(vertex_layer)\n",
        "face_layer = layers.Dense(face_size_weighted * decode_dense_multiplier, activation=\"relu\", name='decode_face_dense_2')(face_layer)\n",
        "\n",
        "vertex_layer = layers.Dense(vertex_input_size * 3, activation=\"relu\", name='decode_vertex_dense_3')(vertex_layer)\n",
        "face_layer = layers.Dense(face_input_size * 3, activation=\"relu\", name='decode_face_dense_3')(face_layer)\n",
        "\n",
        "vertex_layer = layers.Reshape((vertex_input_size, 3), name='vertex_output')(vertex_layer)\n",
        "face_layer = layers.Reshape((face_input_size, 3), name='face_output')(face_layer)\n",
        "\n",
        "decoder = keras.Model(latent_inputs, (vertex_layer, face_layer), name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "62Bsfx3kyH9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4013982-8e4c-4182-d85c-05427231e914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " latent (InputLayer)            [(None, 32)]         0           []                               \n",
            "                                                                                                  \n",
            " from_latent_dense_1 (Dense)    (None, 1569)         51777       ['latent[0][0]']                 \n",
            "                                                                                                  \n",
            " from_latent_dense_2 (Dense)    (None, 2242)         3519940     ['from_latent_dense_1[0][0]']    \n",
            "                                                                                                  \n",
            " from_latent_dense_3 (Dense)    (None, 3203)         7184329     ['from_latent_dense_2[0][0]']    \n",
            "                                                                                                  \n",
            " decode_vertex_dense_1 (Dense)  (None, 2330)         7465320     ['from_latent_dense_3[0][0]']    \n",
            "                                                                                                  \n",
            " decode_face_dense_1 (Dense)    (None, 872)          2793888     ['from_latent_dense_3[0][0]']    \n",
            "                                                                                                  \n",
            " decode_vertex_dense_2 (Dense)  (None, 3329)         7759899     ['decode_vertex_dense_1[0][0]']  \n",
            "                                                                                                  \n",
            " decode_face_dense_2 (Dense)    (None, 1246)         1087758     ['decode_face_dense_1[0][0]']    \n",
            "                                                                                                  \n",
            " decode_vertex_dense_3 (Dense)  (None, 33294)        110869020   ['decode_vertex_dense_2[0][0]']  \n",
            "                                                                                                  \n",
            " decode_face_dense_3 (Dense)    (None, 62322)        77715534    ['decode_face_dense_2[0][0]']    \n",
            "                                                                                                  \n",
            " vertex_output (Reshape)        (None, 11098, 3)     0           ['decode_vertex_dense_3[0][0]']  \n",
            "                                                                                                  \n",
            " face_output (Reshape)          (None, 20774, 3)     0           ['decode_face_dense_3[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 218,447,465\n",
            "Trainable params: 218,447,465\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define VAE class\n",
        "\n",
        "print_shape_debug = False\n",
        "print_loss_debug = True\n",
        "\n",
        "class VAE(keras.Model):\n",
        "  def __init__(self, encoder, decoder, beta=0.1, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.beta = beta\n",
        "    self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "    self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "      name=\"reconstruction_loss\"\n",
        "    )\n",
        "    self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [\n",
        "      self.total_loss_tracker,\n",
        "      self.reconstruction_loss_tracker,\n",
        "      self.kl_loss_tracker,\n",
        "    ]\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Define the forward pass here using the layers defined in init\n",
        "    z_mean, z_log_var, z = self.encoder(inputs)\n",
        "    decoded = self.decoder(z)\n",
        "    return decoded\n",
        "\n",
        "  def train_step(self, data):\n",
        "    with tf.GradientTape() as tape:\n",
        "      data_vertex, data_face = data[0]\n",
        "\n",
        "      check_data_vertex = tf.debugging.check_numerics(data_vertex, \"data_vertex Nan or Inf\")\n",
        "      check_data_face = tf.debugging.check_numerics(data_face, \"data_face Nan or Inf\")\n",
        "\n",
        "      if print_shape_debug:\n",
        "        print(f\"data_vertex : {check_data_vertex}\")\n",
        "        print(f\"data_face : {check_data_face}\")\n",
        "\n",
        "      z_mean, z_log_var, z = self.encoder((data_vertex, data_face))\n",
        "      check_z_mean = tf.debugging.check_numerics(z_mean, \"z_mean Nan or Inf\")\n",
        "      check_z_log_var = tf.debugging.check_numerics(z_log_var, \"z_log_var Nan or Inf\")\n",
        "      check_z = tf.debugging.check_numerics(z_log_var, \"z Nan or Inf\")\n",
        "\n",
        "      if print_shape_debug:\n",
        "        print(f\"z_mean : {check_z_mean}\")\n",
        "        print(f\"z_log_var : {check_z_log_var}\")\n",
        "        print(f\"z : {check_z}\")\n",
        "\n",
        "      reconstruction = self.decoder(z)\n",
        "      reconstruction_vertex, reconstruction_face = reconstruction\n",
        "\n",
        "      check_reconstruction_vertex = tf.debugging.check_numerics(reconstruction_vertex, \"reconstruction_vertex Nan or Inf\")\n",
        "      check_reconstruction_face = tf.debugging.check_numerics(reconstruction_face, \"reconstruction_face Nan or Inf\")\n",
        "\n",
        "      if print_shape_debug:\n",
        "        print(f\"reconstruction_vertex : {check_reconstruction_vertex}\")\n",
        "        print(f\"reconstruction_face : {check_reconstruction_face}\")\n",
        "\n",
        "      mse_loss_vertex = keras.losses.mean_squared_error(data_vertex, reconstruction_vertex)\n",
        "      tf.debugging.check_numerics(mse_loss_vertex, \"mse_loss_vertex Nan or Inf\")\n",
        "      sum_loss_vertex = tf.reduce_sum(mse_loss_vertex, axis=(1))\n",
        "      tf.debugging.check_numerics(sum_loss_vertex, \"sum_loss_vertex Nan or Inf\")\n",
        "      reconstruction_loss_vertex = tf.reduce_mean(sum_loss_vertex)\n",
        "      tf.debugging.check_numerics(reconstruction_loss_vertex, \"reconstruction_loss_vertex Nan or Inf\")\n",
        "\n",
        "      mse_loss_face = keras.losses.mean_squared_error(data_face, reconstruction_face)\n",
        "      tf.debugging.check_numerics(mse_loss_face, \"mse_loss_face Nan or Inf\")\n",
        "      sum_loss_face = tf.reduce_sum(mse_loss_face, axis=(1))\n",
        "      tf.debugging.check_numerics(sum_loss_face, \"sum_loss_face Nan or Inf\")\n",
        "      reconstruction_loss_face = tf.reduce_mean(sum_loss_face)\n",
        "      tf.debugging.check_numerics(reconstruction_loss_face, \"reconstruction_loss_face Nan or Inf\")\n",
        "\n",
        "      reconstruction_loss = reconstruction_loss_vertex + reconstruction_loss_face\n",
        "\n",
        "      kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "      kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "      total_loss = reconstruction_loss + self.beta * kl_loss\n",
        "    grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "    self.total_loss_tracker.update_state(total_loss)\n",
        "    self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "    self.kl_loss_tracker.update_state(kl_loss)\n",
        "    return {\n",
        "      \"loss\": self.total_loss_tracker.result(),\n",
        "      \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "      \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "    }"
      ],
      "metadata": {
        "id": "OYxktNPW1ivM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the VAE with our artifically expanded dataset {vertical-output: true}\n",
        "\n",
        "# model size for 208mil encoder + 218mil decoder = 1.6GB\n",
        "\n",
        "ply_objs = None\n",
        "\n",
        "# Initialize VAE and try one prediction to allow weight import/export\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(config.learning_rate))\n",
        "_ = vae.predict([vertex_input_list[:1], face_input_list[:1]])\n",
        "\n",
        "start_epoch = 0\n",
        "end_epoch = 100000\n",
        "model_checkpoint_frequency = 5000 if use_partial_dataset else 2000\n",
        "\n",
        "if model_use_gdrive:\n",
        "  if use_partial_dataset:\n",
        "    backup_checkpoint_path = DRIVE + \"models/partialdataset_model_weights.h5\"\n",
        "  else:\n",
        "    backup_checkpoint_path = DRIVE + \"models/model_weights.h5\"\n",
        "\n",
        "  try:\n",
        "      vae.load_weights(backup_checkpoint_path)\n",
        "  except FileNotFoundError:\n",
        "      print('Model weights not found.')\n",
        "\n",
        "for i in range(start_epoch, end_epoch, model_checkpoint_frequency):\n",
        "  hist = vae.fit([vertex_input_list, face_input_list], initial_epoch = i,\n",
        "                 epochs = min(i+model_checkpoint_frequency, end_epoch), batch_size = dataset_size, callbacks=[\n",
        "                      WandbMetricsLogger(log_freq=5),\n",
        "                      WandbModelCheckpoint(\"models\")\n",
        "                    ])\n",
        "\n",
        "  vae.save_weights(backup_checkpoint_path)"
      ],
      "metadata": {
        "id": "uVWkGYWH4bgv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f8feeb0-c355-43a6-c211-f319ebf38f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 498ms/step\n",
            "Epoch 1/5000\n",
            "1/1 [==============================] - 26s 26s/step - loss: 20463.6094 - reconstruction_loss: 20461.6953 - kl_loss: 19.1452\n",
            "Epoch 2/5000\n",
            "1/1 [==============================] - 11s 11s/step - loss: 20189.2715 - reconstruction_loss: 20187.3633 - kl_loss: 19.0833\n",
            "Epoch 3/5000\n",
            "1/1 [==============================] - 13s 13s/step - loss: 20095.0723 - reconstruction_loss: 20093.1758 - kl_loss: 18.9657\n",
            "Epoch 4/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 20023.9707 - reconstruction_loss: 20022.0742 - kl_loss: 18.9603\n",
            "Epoch 5/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19970.1387 - reconstruction_loss: 19968.2422 - kl_loss: 18.9630\n",
            "Epoch 6/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19913.7656 - reconstruction_loss: 19911.8672 - kl_loss: 18.9861\n",
            "Epoch 7/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19901.0449 - reconstruction_loss: 19899.1426 - kl_loss: 19.0141\n",
            "Epoch 8/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19863.6543 - reconstruction_loss: 19861.7500 - kl_loss: 19.0505\n",
            "Epoch 9/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19834.2598 - reconstruction_loss: 19832.3516 - kl_loss: 19.0889\n",
            "Epoch 10/5000\n",
            "1/1 [==============================] - 11s 11s/step - loss: 19786.6895 - reconstruction_loss: 19784.7734 - kl_loss: 19.1544\n",
            "Epoch 11/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19764.2891 - reconstruction_loss: 19762.3672 - kl_loss: 19.2215\n",
            "Epoch 12/5000\n",
            "1/1 [==============================] - 11s 11s/step - loss: 19733.5098 - reconstruction_loss: 19731.5781 - kl_loss: 19.3121\n",
            "Epoch 13/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19717.6875 - reconstruction_loss: 19715.7461 - kl_loss: 19.4157\n",
            "Epoch 14/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19699.8770 - reconstruction_loss: 19697.9219 - kl_loss: 19.5483\n",
            "Epoch 15/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19678.1211 - reconstruction_loss: 19676.1523 - kl_loss: 19.6855\n",
            "Epoch 16/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19668.3652 - reconstruction_loss: 19666.3828 - kl_loss: 19.8280\n",
            "Epoch 17/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19639.9785 - reconstruction_loss: 19637.9805 - kl_loss: 19.9881\n",
            "Epoch 18/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19622.5898 - reconstruction_loss: 19620.5742 - kl_loss: 20.1475\n",
            "Epoch 19/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19614.9414 - reconstruction_loss: 19612.9102 - kl_loss: 20.3089\n",
            "Epoch 20/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19595.9883 - reconstruction_loss: 19593.9395 - kl_loss: 20.4789\n",
            "Epoch 21/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19585.9570 - reconstruction_loss: 19583.8945 - kl_loss: 20.6263\n",
            "Epoch 22/5000\n",
            "1/1 [==============================] - 7s 7s/step - loss: 19573.9199 - reconstruction_loss: 19571.8438 - kl_loss: 20.7542\n",
            "Epoch 23/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19566.1797 - reconstruction_loss: 19564.0938 - kl_loss: 20.8560\n",
            "Epoch 24/5000\n",
            "1/1 [==============================] - 17s 17s/step - loss: 19548.4277 - reconstruction_loss: 19546.3340 - kl_loss: 20.9387\n",
            "Epoch 25/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19532.7715 - reconstruction_loss: 19530.6699 - kl_loss: 21.0099\n",
            "Epoch 26/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19523.0137 - reconstruction_loss: 19520.9062 - kl_loss: 21.0710\n",
            "Epoch 27/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19508.7969 - reconstruction_loss: 19506.6836 - kl_loss: 21.1328\n",
            "Epoch 28/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19499.9590 - reconstruction_loss: 19497.8398 - kl_loss: 21.1912\n",
            "Epoch 29/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19489.4570 - reconstruction_loss: 19487.3320 - kl_loss: 21.2532\n",
            "Epoch 30/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19480.7637 - reconstruction_loss: 19478.6328 - kl_loss: 21.3098\n",
            "Epoch 31/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19476.6172 - reconstruction_loss: 19474.4805 - kl_loss: 21.3623\n",
            "Epoch 32/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19466.0098 - reconstruction_loss: 19463.8672 - kl_loss: 21.4191\n",
            "Epoch 33/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19462.8867 - reconstruction_loss: 19460.7383 - kl_loss: 21.4819\n",
            "Epoch 34/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19452.6758 - reconstruction_loss: 19450.5195 - kl_loss: 21.5553\n",
            "Epoch 35/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19450.1055 - reconstruction_loss: 19447.9414 - kl_loss: 21.6353\n",
            "Epoch 36/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19444.6523 - reconstruction_loss: 19442.4805 - kl_loss: 21.7220\n",
            "Epoch 37/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19436.0234 - reconstruction_loss: 19433.8418 - kl_loss: 21.8092\n",
            "Epoch 38/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19431.8691 - reconstruction_loss: 19429.6797 - kl_loss: 21.8952\n",
            "Epoch 39/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19427.5898 - reconstruction_loss: 19425.3926 - kl_loss: 21.9784\n",
            "Epoch 40/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19421.7617 - reconstruction_loss: 19419.5547 - kl_loss: 22.0630\n",
            "Epoch 41/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19416.4395 - reconstruction_loss: 19414.2246 - kl_loss: 22.1473\n",
            "Epoch 42/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19410.1758 - reconstruction_loss: 19407.9531 - kl_loss: 22.2313\n",
            "Epoch 43/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19406.6406 - reconstruction_loss: 19404.4102 - kl_loss: 22.3090\n",
            "Epoch 44/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19401.9688 - reconstruction_loss: 19399.7305 - kl_loss: 22.3776\n",
            "Epoch 45/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19400.0469 - reconstruction_loss: 19397.8027 - kl_loss: 22.4422\n",
            "Epoch 46/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19394.1992 - reconstruction_loss: 19391.9492 - kl_loss: 22.5067\n",
            "Epoch 47/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19391.5645 - reconstruction_loss: 19389.3066 - kl_loss: 22.5712\n",
            "Epoch 48/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19384.6465 - reconstruction_loss: 19382.3828 - kl_loss: 22.6390\n",
            "Epoch 49/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19381.0117 - reconstruction_loss: 19378.7422 - kl_loss: 22.7019\n",
            "Epoch 50/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19378.1777 - reconstruction_loss: 19375.9023 - kl_loss: 22.7601\n",
            "Epoch 51/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19375.5957 - reconstruction_loss: 19373.3145 - kl_loss: 22.8191\n",
            "Epoch 52/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19371.0254 - reconstruction_loss: 19368.7383 - kl_loss: 22.8782\n",
            "Epoch 53/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19369.2520 - reconstruction_loss: 19366.9570 - kl_loss: 22.9410\n",
            "Epoch 54/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19365.2031 - reconstruction_loss: 19362.9023 - kl_loss: 23.0104\n",
            "Epoch 55/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19360.2969 - reconstruction_loss: 19357.9883 - kl_loss: 23.0802\n",
            "Epoch 56/5000\n",
            "1/1 [==============================] - 11s 11s/step - loss: 19359.5156 - reconstruction_loss: 19357.2012 - kl_loss: 23.1507\n",
            "Epoch 57/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19354.9648 - reconstruction_loss: 19352.6406 - kl_loss: 23.2354\n",
            "Epoch 58/5000\n",
            "1/1 [==============================] - 11s 11s/step - loss: 19351.3965 - reconstruction_loss: 19349.0645 - kl_loss: 23.3223\n",
            "Epoch 59/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19347.7773 - reconstruction_loss: 19345.4355 - kl_loss: 23.4087\n",
            "Epoch 60/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19343.6191 - reconstruction_loss: 19341.2695 - kl_loss: 23.4915\n",
            "Epoch 61/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19342.6953 - reconstruction_loss: 19340.3379 - kl_loss: 23.5729\n",
            "Epoch 62/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19339.1875 - reconstruction_loss: 19336.8223 - kl_loss: 23.6581\n",
            "Epoch 63/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19336.9902 - reconstruction_loss: 19334.6172 - kl_loss: 23.7397\n",
            "Epoch 64/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19333.9941 - reconstruction_loss: 19331.6133 - kl_loss: 23.8130\n",
            "Epoch 65/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19328.2617 - reconstruction_loss: 19325.8730 - kl_loss: 23.8920\n",
            "Epoch 66/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19326.9961 - reconstruction_loss: 19324.5996 - kl_loss: 23.9726\n",
            "Epoch 67/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19323.1836 - reconstruction_loss: 19320.7773 - kl_loss: 24.0580\n",
            "Epoch 68/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19319.4004 - reconstruction_loss: 19316.9844 - kl_loss: 24.1522\n",
            "Epoch 69/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19315.7812 - reconstruction_loss: 19313.3555 - kl_loss: 24.2520\n",
            "Epoch 70/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19312.8184 - reconstruction_loss: 19310.3828 - kl_loss: 24.3505\n",
            "Epoch 71/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19311.4258 - reconstruction_loss: 19308.9805 - kl_loss: 24.4467\n",
            "Epoch 72/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19308.2773 - reconstruction_loss: 19305.8242 - kl_loss: 24.5385\n",
            "Epoch 73/5000\n",
            "1/1 [==============================] - 11s 11s/step - loss: 19304.5469 - reconstruction_loss: 19302.0859 - kl_loss: 24.6184\n",
            "Epoch 74/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19300.8574 - reconstruction_loss: 19298.3867 - kl_loss: 24.7061\n",
            "Epoch 75/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19295.1172 - reconstruction_loss: 19292.6367 - kl_loss: 24.8004\n",
            "Epoch 76/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19293.7383 - reconstruction_loss: 19291.2500 - kl_loss: 24.8844\n",
            "Epoch 77/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19290.9609 - reconstruction_loss: 19288.4648 - kl_loss: 24.9647\n",
            "Epoch 78/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19287.2773 - reconstruction_loss: 19284.7715 - kl_loss: 25.0633\n",
            "Epoch 79/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19284.1094 - reconstruction_loss: 19281.5918 - kl_loss: 25.1816\n",
            "Epoch 80/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19282.5078 - reconstruction_loss: 19279.9766 - kl_loss: 25.3039\n",
            "Epoch 81/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19276.2539 - reconstruction_loss: 19273.7109 - kl_loss: 25.4203\n",
            "Epoch 82/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19272.1152 - reconstruction_loss: 19269.5625 - kl_loss: 25.5270\n",
            "Epoch 83/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19269.8281 - reconstruction_loss: 19267.2637 - kl_loss: 25.6354\n",
            "Epoch 84/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19265.3203 - reconstruction_loss: 19262.7441 - kl_loss: 25.7579\n",
            "Epoch 85/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19262.6895 - reconstruction_loss: 19260.1016 - kl_loss: 25.8818\n",
            "Epoch 86/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19259.3594 - reconstruction_loss: 19256.7598 - kl_loss: 26.0049\n",
            "Epoch 87/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19255.6094 - reconstruction_loss: 19252.9961 - kl_loss: 26.1353\n",
            "Epoch 88/5000\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19251.0957 - reconstruction_loss: 19248.4707 - kl_loss: 26.2575\n",
            "Epoch 89/5000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19247.9004 - reconstruction_loss: 19245.2617 - kl_loss: 26.3826\n",
            "Epoch 90/5000\n",
            "1/1 [==============================] - 13s 13s/step - loss: 19244.0859 - reconstruction_loss: 19241.4336 - kl_loss: 26.5181\n",
            "Epoch 91/5000\n",
            "1/1 [==============================] - 14s 14s/step - loss: 19238.9414 - reconstruction_loss: 19236.2773 - kl_loss: 26.6436\n",
            "Epoch 92/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19235.9629 - reconstruction_loss: 19233.2871 - kl_loss: 26.7668\n",
            "Epoch 93/5000\n",
            "1/1 [==============================] - 9s 9s/step - loss: 19232.7051 - reconstruction_loss: 19230.0156 - kl_loss: 26.8853\n",
            "Epoch 94/5000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4248a701f2fc>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   hist = vae.fit([vertex_input_list, face_input_list], initial_epoch = i,\n\u001b[0m\u001b[1;32m     43\u001b[0m                  epochs = min(i+model_checkpoint_frequency, end_epoch), batch_size = dataset_size, callbacks=[tensorboard])\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to run if training is halted via interrupt\n",
        "if model_use_gdrive:\n",
        "  if use_partial_dataset:\n",
        "    backup_checkpoint_path = DRIVE + \"models/partialdataset_model_weights.h5\"\n",
        "  else:\n",
        "    backup_checkpoint_path = DRIVE + \"models/model_weights.h5\""
      ],
      "metadata": {
        "id": "3fTubNLJBgOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export model output as PLY file {vertical-output: true}\n",
        "\n",
        "#todo: still using input_list as fake model output\n",
        "vertex_output = vertex_input_list[0]\n",
        "face_output = face_input_list[0]\n",
        "\n",
        "# De-normalize the output data\n",
        "vertex_output = vertex_output * tf.sqrt(variance_ver) + mean_ver\n",
        "face_output = face_output * tf.sqrt(variance_face) + mean_face\n",
        "\n",
        "print(f'Mean of vertex_input_list values: {np.mean(vertex_output)}, Std Dev: {np.std(vertex_output)}')\n",
        "print(f'\\nMean of face_input_list values: {np.mean(face_output)}, Std Dev: {np.std(face_output)}')\n",
        "\n",
        "obj_from_model = PlyObject.from_model(\"from_tf_arr\", vertex_output[0], face_output[0], 0)\n",
        "\n",
        "obj_from_model.save_file(f\"/content/{obj_from_model.name}.ply\")"
      ],
      "metadata": {
        "id": "4Zri8dw8AQcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6ef694-8ca2-48ab-9bb5-3b29c47aba3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of vertex_input_list values: 0.3851780891418457, Std Dev: 0.34732112288475037\n",
            "\n",
            "Mean of face_input_list values: 5532.9873046875, Std Dev: 3210.900146484375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Side Adventure: Characterize performance for random list generation"
      ],
      "metadata": {
        "id": "qcuWgkpxV0rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title {vertical-output: true}\n",
        "\n",
        "# algorithm specification:\n",
        "# generate an evenly distributed random list that sums to total_sum, minimum value of any element = 1\n",
        "\n",
        "# this will be used extensively during padding, so it's worth spending time optimizing and performance testing\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def verify_random_list(lst, length, total_sum):\n",
        "  if len(lst) != length:\n",
        "    raise ValueError(\"Incorrect length\")\n",
        "  elif sum(lst) != total_sum:\n",
        "    raise ValueError(\"Incorrect sum\")\n",
        "  elif min(lst) < 1:\n",
        "    raise ValueError(\"Minimum value not 1\")\n",
        "\n",
        "\n",
        "def random_list_simple(length, total_sum):\n",
        "  list = [1]*length\n",
        "\n",
        "  # Distribute the total_sum across the list\n",
        "  for i in range(total_sum - length):\n",
        "    list[random.randint(0, length - 1)] += 1\n",
        "\n",
        "  return list\n",
        "\n",
        "\n",
        "def random_list_dist_fill_float(length, total_sum, fill_float):\n",
        "  # Pre-allocate the list to the target length\n",
        "  list = [0]*length\n",
        "\n",
        "  # Range for random value generation for each element\n",
        "  upper_bound = int(fill_float * ((total_sum - length) / length))\n",
        "\n",
        "  if upper_bound < 1:\n",
        "    return random_list_simple(length, total_sum)\n",
        "\n",
        "  # Generate initial list and calculate the current sum\n",
        "  current_sum = 0\n",
        "  for i in range(length):\n",
        "    list[i] = random.randint(1, upper_bound)\n",
        "    current_sum += list[i]\n",
        "\n",
        "  # If current_sum already exceed total_sum, retry the function\n",
        "  if current_sum > total_sum:\n",
        "    return random_list_dist_fill_float(length, total_sum, fill_float)\n",
        "\n",
        "  # Distribute the remaining sum across the list\n",
        "  for i in range(current_sum, total_sum):\n",
        "    list[random.randint(0, length - 1)] += 1\n",
        "\n",
        "  return list\n",
        "\n",
        "\n",
        "def random_list_try_dist_then_simple(length, total_sum, fill_float):\n",
        "  # Pre-allocate the list to the target length\n",
        "  list = [0]*length\n",
        "\n",
        "  # Range for random value generation for each element\n",
        "  upper_bound = int(fill_float * ((total_sum - length) / length))\n",
        "\n",
        "  if upper_bound < 1:\n",
        "    return random_list_simple(length, total_sum)\n",
        "\n",
        "  # Generate initial list and calculate the current sum\n",
        "  current_sum = 0\n",
        "  for i in range(length):\n",
        "    list[i] = random.randint(1, upper_bound)\n",
        "    current_sum += list[i]\n",
        "\n",
        "  # If current_sum already exceed total_sum, retry the function\n",
        "  if current_sum > total_sum:\n",
        "    return random_list_simple(length, total_sum)\n",
        "\n",
        "  # Distribute the remaining sum across the list\n",
        "  for i in range(current_sum, total_sum):\n",
        "    list[random.randint(0, length - 1)] += 1\n",
        "\n",
        "  return list\n",
        "\n",
        "\n",
        "def random_list_np_multinomal(length, total_sum):\n",
        "  adjusted_sum = total_sum - length\n",
        "  result = np.random.multinomial(adjusted_sum, np.ones(length)/length) + 1\n",
        "\n",
        "  return result.tolist()\n",
        "\n",
        "\n",
        "scenarios = {\n",
        "  \"Small Scale 10x\": (100, 1000),\n",
        "  \"Large Scale 5x\": (4000, 21000),\n",
        "  \"Large Scale 2x\": (10500, 21000),\n",
        "  \"Large Scale 1.3x\": (16000, 21000),\n",
        "}\n",
        "\n",
        "functions = [\n",
        "  random_list_simple,\n",
        "  random_list_np_multinomal\n",
        "]\n",
        "\n",
        "float_functions = [\n",
        "  random_list_dist_fill_float,\n",
        "  random_list_try_dist_then_simple,\n",
        "]\n",
        "\n",
        "num_iterations = 100\n",
        "\n",
        "float_values = [1.6, 1.8, 1.95, 2.0]\n",
        "\n",
        "for scenario_name, scenario_values in scenarios.items():\n",
        "  print(f\"Scenario: {scenario_name} (repeated {num_iterations} iterations)\")\n",
        "  for func in functions:\n",
        "    start_time = time.time()\n",
        "\n",
        "    results = []\n",
        "    errors = set()\n",
        "    for _ in range(num_iterations):\n",
        "      try:\n",
        "        res = func(*scenario_values)\n",
        "        results.append(res)\n",
        "        verify_random_list(res, *scenario_values)\n",
        "      except ValueError as e:\n",
        "        errors.add(str(e))\n",
        "\n",
        "    if not errors:\n",
        "      results_df = pd.DataFrame(results)\n",
        "      std_dev = results_df.std()\n",
        "      avg_std_dev = np.mean(std_dev)\n",
        "      max_value = results_df.max().max()\n",
        "      min_value = results_df.min().min()\n",
        "      times_max_occurred = results_df.eq(max_value).sum().sum()\n",
        "      times_min_occurred = results_df.eq(min_value).sum().sum()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    error_message = 'Failed' if errors else 'Succeeded'\n",
        "\n",
        "    print(f\"\\n{func.__name__} ran in {elapsed_time:.2f} s. {error_message}.\")\n",
        "    if errors:\n",
        "      print(f\"Errors: {', '.join(errors)}\")\n",
        "    else:\n",
        "      print(f\"Average standard deviation across iterations: {avg_std_dev:.2f}\")\n",
        "      print(f\"Max value: {max_value} (occurred {times_max_occurred} times)\")\n",
        "      print(f\"Min value: {min_value} (occurred {times_min_occurred} times)\")\n",
        "\n",
        "  # try float functions with different float values\n",
        "  for float_func in float_functions:\n",
        "    for float_value in float_values:\n",
        "      print()\n",
        "      start_time = time.time()\n",
        "\n",
        "      results = []\n",
        "      errors = set()\n",
        "      for _ in range(num_iterations):\n",
        "        try:\n",
        "          res = float_func(*scenario_values, float_value)\n",
        "          results.append(res)\n",
        "          verify_random_list(res, *scenario_values)\n",
        "        except ValueError as e:\n",
        "          errors.add(str(e))\n",
        "\n",
        "      if not errors:\n",
        "        results_df = pd.DataFrame(results)\n",
        "        std_dev = results_df.std()\n",
        "        avg_std_dev = np.mean(std_dev)\n",
        "        max_value = results_df.max().max()\n",
        "        min_value = results_df.min().min()\n",
        "        times_max_occurred = results_df.eq(max_value).sum().sum()\n",
        "        times_min_occurred = results_df.eq(min_value).sum().sum()\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      error_message = 'Failed' if errors else 'Succeeded'\n",
        "\n",
        "      print(f\"{float_func.__name__} (float {float_value}) ran in {elapsed_time:.2f} s. {error_message}.\")\n",
        "      if errors:\n",
        "        print(f\"{', '.join(errors)}\")\n",
        "      else:\n",
        "        print(f\"Average standard deviation across iterations: {avg_std_dev:.2f}\")\n",
        "        print(f\"Max value: {max_value} (occurred {times_max_occurred} times)\")\n",
        "        print(f\"Min value: {min_value} (occurred {times_min_occurred} times)\")\n",
        "\n",
        "  print('-' * 50)"
      ],
      "metadata": {
        "id": "Y1o1MDGcR2eQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}